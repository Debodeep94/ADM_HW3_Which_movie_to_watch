{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.1] Get the list of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/10_to_Midnight'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies2.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "urls = []\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies1.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "lst_a\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[10000]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Z.P.G.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies3.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "lst_a\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[20000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Whistle_(2003_film)'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[29999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicturls = {}\n",
    "for i in range(len(urls)):\n",
    "    dicturls[i] = urls[i]\n",
    "with open('dicturls.json', 'w') as fp:\n",
    "    json.dump(dicturls, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Whistle_(2003_film)'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicturls[str(29999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.2] Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "import time\n",
    "def getwikipageshtml(urls):\n",
    "    k=0\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            ur_l = requests.get(urls[i])\n",
    "            soup = BeautifulSoup(ur_l.content, 'html.parser')\n",
    "            soup = soup.prettify(\"utf-8\")   \n",
    "            stringa = 'Articles/article-'+str(k)+'.html'\n",
    "            k = k+1\n",
    "            Html_file= open(stringa, \"wb\")\n",
    "            Html_file.write(soup)\n",
    "            Html_file.close()\n",
    "        except(URLError,HTTPError, ContentTooShortError)  as e:\n",
    "            html = None\n",
    "        #time.sleep(0.001) #Actually for this task we don't need to stop anytime\n",
    "    return\n",
    "getwikipageshtml(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.3] Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open(\"Articles/article-0.html\")\n",
    "#soup = BeautifulSoup(myfile, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "for i in range(30000):\n",
    "    filename = \"Articles/article-\"+str(i)+\".html\"\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        #Now That I opened the file I have to look for the section asked\n",
    "        #first I take the title and clear him of spaces and the word -Wikipedia\n",
    "        titlepage = soup.title.string\n",
    "        titleonly = titlepage.split(\"- Wikipedia\")\n",
    "        titlepage = titleonly[0].strip()\n",
    "        #now I create empty string as intro and plot\n",
    "        intro = ''\n",
    "        plot = ''\n",
    "        #Now i searcvh the first paragraph that usually or is empty or is the intro\n",
    "        start = soup.find('p')\n",
    "        intro = start\n",
    "        intro1 = start.text\n",
    "        B = ''\n",
    "        #in B I put m,y limit for the paragraphs in the intro, because after this h2 there will always be the plot\n",
    "        B = intro.find_next_sibling('h2')\n",
    "        if(B!=None and B.find_next_sibling('p')):\n",
    "            C = B.find_next_sibling('p')\n",
    "            while(C != intro.find_next_sibling('p')): \n",
    "                intro1 = intro1 + intro.find_next_sibling('p').text\n",
    "                intro = intro.find_next_sibling('p')\n",
    "            plot = ''    \n",
    "            #then i do the same with the plot, so I start at B and end in the next h2\n",
    "            plot = B\n",
    "            plot1 = ''\n",
    "            compare = ''\n",
    "            if(B.find_next_sibling('h2')):\n",
    "                compare = B.find_next_sibling('h2')\n",
    "                compareto = compare.find_next_sibling('p')\n",
    "                while(compareto != plot.find_next_sibling('p')):\n",
    "                    plot1 = plot1 + plot.find_next_sibling('p').text\n",
    "                    #print(plot1)\n",
    "                    plot = plot.find_next_sibling('p')\n",
    "                    #if plot or intro are empty I put NA\n",
    "        if(intro1 == ''):\n",
    "            intro1 = \"NA\"\n",
    "        if plot1 == '':\n",
    "            plot1 = \"NA\"\n",
    "        #Now I start working on the other features\n",
    "        intro = intro1\n",
    "        plot = plot1\n",
    "        film_name = 'NA'\n",
    "        director = \"NA\"\n",
    "        producer = \"NA\"\n",
    "        writer = \"NA\"\n",
    "        starring = \"NA\"\n",
    "        music = \"NA\"\n",
    "        release_date = \"NA\"\n",
    "        runtime = \"NA\"\n",
    "        country = \"NA\"\n",
    "        language = \"NA\"\n",
    "        budget = \"NA\"\n",
    "#'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'\n",
    "        for link in soup.find_all('tr'):\n",
    "            if soup.find('th',{'class': ['summary']})!= None:\n",
    "                    film_name = soup.find('th',{'class': ['summary']} ).text.strip()\n",
    "            if link.th:\n",
    "#I just check in the th and if I find the class I need I take the relative td. Some of them are inaccessible so the if link.td\n",
    "                if(link.th.text.strip() == 'Directed by'):\n",
    "                     director = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Produced by'):\n",
    "                      producer = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Written by'):\n",
    "                    writer = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Starring'):\n",
    "                    starring = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Music by'):\n",
    "                     music = link.td.text.strip()               \n",
    "                elif(link.th.text.strip() == 'Release date'):\n",
    "                    if link.td:\n",
    "                        release_date = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Running time'):\n",
    "                    runtime = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Country'):\n",
    "                    if link.td:\n",
    "                        country = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Language'):\n",
    "                    if link.td:\n",
    "                        language = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Budget'):\n",
    "                    budget = link.td.text.strip()\n",
    "#now I open the tsv files and create one for every film.        \n",
    "        tsvname = 'Tsvfiles/'+'film'+str(i)+'.tsv'\n",
    "        with io.open(tsvname, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['title','intro', 'plot', 'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'])\n",
    "            tsv_writer.writerow([titlepage, intro, plot, film_name, director, producer, writer, starring, music, release_date, runtime, \n",
    "                 country, language, budget])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whistle (2003 film)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n         What Became of Jack and Jill?\\n        \\n\\n       is a 1972 British\\n       \\n        horror film\\n       \\n       directed by\\n       \\n        Bill Bain\\n       \\n       and starring\\n       \\n        Mona Washbourne\\n       \\n       ,\\n       \\n        Paul Nicholas\\n       \\n       , and\\n       \\n        Vanessa Howard\\n       \\n       .\\n       \\n\\n         [1]\\n        \\n\\n       It was part of an abandoned attempt by\\n       \\n        Amicus Pictures\\n       \\n       to compete with\\n       \\n        Hammer Studios\\n       \\n       by breaking into the\\n       \\n        grindhouse\\n       \\n       market. Studio executives were ultimately too disturbed by the final product to release it under the Amicus name, and they sold the film to\\n       \\n        20th Century Fox\\n       \\n       .\\n      '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is just to try stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\leona\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\leona\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>plot</th>\n",
       "      <th>film_name</th>\n",
       "      <th>director</th>\n",
       "      <th>producer</th>\n",
       "      <th>writer</th>\n",
       "      <th>starring</th>\n",
       "      <th>music</th>\n",
       "      <th>release date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>country</th>\n",
       "      <th>language</th>\n",
       "      <th>budget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>big chill film</td>\n",
       "      <td>big chill 1983 american comedi drama film dire...</td>\n",
       "      <td>harold cooper bath young son wife dr sarah coo...</td>\n",
       "      <td>big chill</td>\n",
       "      <td>lawrenc kasdan</td>\n",
       "      <td>michael shamberg</td>\n",
       "      <td>lawrenc kasdan barbara benedek</td>\n",
       "      <td>tom bereng glenn close jeff goldblum william h...</td>\n",
       "      <td>bill conti</td>\n",
       "      <td>septemb 28 1983 1983 09 28</td>\n",
       "      <td>105 minut</td>\n",
       "      <td>unit state</td>\n",
       "      <td>english</td>\n",
       "      <td>8 million 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            title                                              intro  \\\n",
       "0  big chill film  big chill 1983 american comedi drama film dire...   \n",
       "\n",
       "                                                plot  film_name  \\\n",
       "0  harold cooper bath young son wife dr sarah coo...  big chill   \n",
       "\n",
       "         director          producer                          writer  \\\n",
       "0  lawrenc kasdan  michael shamberg  lawrenc kasdan barbara benedek   \n",
       "\n",
       "                                            starring       music  \\\n",
       "0  tom bereng glenn close jeff goldblum william h...  bill conti   \n",
       "\n",
       "                 release date    runtime     country language       budget  \n",
       "0  septemb 28 1983 1983 09 28  105 minut  unit state  english  8 million 1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset1 = pd.read_csv('Cleantsv/filmclean-8.tsv', delimiter='\\t')\n",
    "#dataset1 = pd.read_csv('Tsvfiles/film1', delimiter='\\t')\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'budget 10 midnight'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I clean The files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "#I use the function on every section of the tsv files, except for the tiles of the categories, so from the part 13\n",
    "for i in range(30000):\n",
    "    file1 = open('Tsvfiles/film'+str(i)+'.tsv', encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    words = line.split('\\t') \n",
    "    for j in range(13, len(words)):\n",
    "        words[j] = preprocess(words[j])\n",
    "        if j ==13:\n",
    "            #Here for the format of tsv files and my split('\\t') the word budget would always be in my title, so I take her out\n",
    "            words[j] = words[j].replace('budget ','')    \n",
    "    tsvname = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    with io.open(tsvname, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['title','intro', 'plot', 'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'])\n",
    "            tsv_writer.writerow(words[13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whistl 2003 film'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionar = {}\n",
    "k = 0\n",
    "for i in range(30000):\n",
    "    file1 = open(\"Cleantsv/filmclean-\"+str(i)+'.tsv', encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    #print(wordssplitted1, wordssplitted2)\n",
    "    for i in wordssplitted1:\n",
    "        #print(type(i))\n",
    "        if i not in dictionar:\n",
    "            dictionar[i] = str(k)\n",
    "            k = k+1\n",
    "    for i in wordssplitted2:\n",
    "        #print(type(i))\n",
    "        if i not in dictionar:\n",
    "            dictionar[i] = str(k)\n",
    "            k = k+1\n",
    "#dictionar\n",
    "import json\n",
    "\n",
    "with open('Dictionary.json', 'w') as fp:\n",
    "    json.dump(dictionar, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12741'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data)\n",
    "#dictions = pd.DataFrame(diction)\n",
    "diction['1913']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diction)\n",
    "type(diction['hom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the inverted Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionar2 = {}\n",
    "length = 0\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    #print(wordssplitted1, wordssplitted2)\n",
    "    for j in wordssplitted1:\n",
    "        code = diction[j]\n",
    "        if code not in dictionar2:\n",
    "            dictionar2[code] = [file]\n",
    "        elif file not in dictionar2[code]:\n",
    "            dictionar2[code].append(file)\n",
    "    for j in wordssplitted2:\n",
    "        code = diction[j]\n",
    "        if code not in dictionar2:\n",
    "            dictionar2[code] = [file]\n",
    "        elif file not in dictionar2[code]:\n",
    "            dictionar2[code].append(file)\n",
    "#dictionar\n",
    "import json\n",
    "\n",
    "with open('Dictionary1.json', 'w') as fp:\n",
    "    json.dump(dictionar2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)\n",
    "#dictions = pd.DataFrame(diction)\n",
    "#diction2['0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diction['2019']\n",
    "len(diction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#diction2['7507']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First engine searchengine1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ross\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Superman III</td>\n",
       "      <td>Superman III               is a Briti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Superman_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Footloose (1984 film)</td>\n",
       "      <td>Footloose               is a 1984 Ame...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Footloose_(1984_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Protocol (film)</td>\n",
       "      <td>Protocol               is a 1984 Amer...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Protocol_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Splash (film)</td>\n",
       "      <td>Splash               is a 1984 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Splash_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday the 13th: A New Beginning</td>\n",
       "      <td>Friday the 13th: A New Beginning     ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Friday_the_13th:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Turning Paige</td>\n",
       "      <td>Turning Paige               is a 2001...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Turning_Paige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>The Saddest Music in the World</td>\n",
       "      <td>The Saddest Music in the World       ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Saddest_Musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Goon (film)</td>\n",
       "      <td>Goon               is a 2011 Canadian...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Goon_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Charming (film)</td>\n",
       "      <td>Charming               is a 2018 Cana...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Charming_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Goon: Last of the Enforcers</td>\n",
       "      <td>Goon: Last of the Enforcers          ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Goon:_Last_of_th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title  \\\n",
       "0                        Superman III   \n",
       "1               Footloose (1984 film)   \n",
       "2                     Protocol (film)   \n",
       "3                       Splash (film)   \n",
       "4    Friday the 13th: A New Beginning   \n",
       "..                                ...   \n",
       "351                     Turning Paige   \n",
       "352    The Saddest Music in the World   \n",
       "353                       Goon (film)   \n",
       "354                   Charming (film)   \n",
       "355       Goon: Last of the Enforcers   \n",
       "\n",
       "                                                 Intro  \\\n",
       "0             Superman III               is a Briti...   \n",
       "1             Footloose               is a 1984 Ame...   \n",
       "2             Protocol               is a 1984 Amer...   \n",
       "3             Splash               is a 1984 Americ...   \n",
       "4             Friday the 13th: A New Beginning     ...   \n",
       "..                                                 ...   \n",
       "351           Turning Paige               is a 2001...   \n",
       "352           The Saddest Music in the World       ...   \n",
       "353           Goon               is a 2011 Canadian...   \n",
       "354           Charming               is a 2018 Cana...   \n",
       "355           Goon: Last of the Enforcers          ...   \n",
       "\n",
       "                                                   Url  \n",
       "0           https://en.wikipedia.org/wiki/Superman_III  \n",
       "1    https://en.wikipedia.org/wiki/Footloose_(1984_...  \n",
       "2        https://en.wikipedia.org/wiki/Protocol_(film)  \n",
       "3          https://en.wikipedia.org/wiki/Splash_(film)  \n",
       "4    https://en.wikipedia.org/wiki/Friday_the_13th:...  \n",
       "..                                                 ...  \n",
       "351        https://en.wikipedia.org/wiki/Turning_Paige  \n",
       "352  https://en.wikipedia.org/wiki/The_Saddest_Musi...  \n",
       "353          https://en.wikipedia.org/wiki/Goon_(film)  \n",
       "354      https://en.wikipedia.org/wiki/Charming_(film)  \n",
       "355  https://en.wikipedia.org/wiki/Goon:_Last_of_th...  \n",
       "\n",
       "[356 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HERE i DEFINE THE FUCTION TO RECEIVE THE DATASET, GIVEN AN INPUT\n",
    "y = list(input().split())\n",
    "def searchengine1(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictiionary based on the input\n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        #print(yfinal)\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "#print(starting_values)\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "        #print(codes)\n",
    "            new = []\n",
    "            for film in final_values:\n",
    "            #print(film)\n",
    "                if film in diction2[yfinal[codes]]:\n",
    "                    new.append(film)\n",
    "            final_values = new\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url'])\n",
    "    #megaDataframe\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:           \n",
    "            k=0\n",
    "            for document in final_values:\n",
    "                totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                totakeurl = str(int(totakeurl.replace('.tsv', '')))\n",
    "                url = dicturls[totakeurl]\n",
    "                temporary = pd.read_csv('Tsvfiles/'+'film'+(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                title = temporary['title'][0]\n",
    "                intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                new_row = [title, intro, url]\n",
    "                megaDataframe.loc[k]=new_row\n",
    "                k=k+1\n",
    "            return megaDataframe\n",
    "A = searchengine1(y)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building of the second search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)\n",
    "#print(Docum_and_words) \n",
    "#I have the lis of documents with intro and plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary with idf to save computational costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary declarations to run the code at any moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 midnight 1983 american crime horror thriller film 3 direct j lee thompson screenplay origin written william robert film star charl bronson lead role support cast includ lisa eilbach andrew steven gene davi geoffrey lewi wilford brimley 10 midnight releas citi film subsidiari cannon film american cinema march 11 1983 warren staci gene davi young offic equip repairman kill women reject sexual advanc attempt flirt alway seen creepi women result frequent reject 4 first victim betti june gilbert offic worker acquaint track wood area observ sex boyfriend ambush coupl kill boyfriend give chase nake woman catch stab death 4 two lo angel polic detect leo kessler charl bronson paul mcann andrew steven investig murder kessler season veteran forc mcann consider younger 4 staci avoid prosecut construct sound alibi assault victim nake except pair latex glove hide fingerprint thu minim evid lauri kessler lisa eilbach daughter leo acquaint victim student nurs becom target killer 4 mcann refus go along kessler plant evid order frame suspect staci goe anoth rampag kill three nurs student friend kessler daughter eventu caught stark nake street staci boast say thing prove crazi hear voic order thing etc one day back street kessler well whole fuck world hear kessler repli shoot staci forehead execut leav consider asid kessler stand bodi surround polic'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Docum_and_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data) #first dict with every word and a unique value\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)#first inverted dict with number and list of document with a word that has that number in diction\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc.split())\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / float(n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncontain = {}\n",
    "for i in diction:\n",
    "    refer = diction[i]\n",
    "    #print(i)\n",
    "    ncontain[i] = len(diction2[refer])\n",
    "with open('ncontain.json', 'w') as fp:\n",
    "    json.dump(ncontain, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\ncontain.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "ncontain = json.loads(data) #first dict with every word and a unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ncontain.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "idfdict = {}\n",
    "for i in ncontain:\n",
    "    idfdict[i]=math.log(30000 / ncontain[i])\n",
    "with open('idfdict.json', 'w') as fp:\n",
    "    json.dump(idfdict, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\idfdict.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "idfdict = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\idfdict.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "idfdict = json.loads(data)\n",
    "#idfdict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second dictionary with the tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionar3 = {}\n",
    "#length = 0\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    temp = []\n",
    "    for j in Docum_and_words[i].split():\n",
    "            if j not in temp:\n",
    "                #dicus = {}\n",
    "                code = diction[j]\n",
    "                value = tf(j, Docum_and_words[i].split())*idfdict[j]\n",
    "                li = [file, value]\n",
    "                if code not in dictionar3:\n",
    "                    #dicus[file]=value\n",
    "                    dictionar3[code] = [li]\n",
    "                else:   \n",
    "                    #dicus[file]=value\n",
    "                    dictionar3[code].append(li)\n",
    "                temp.append(j)\n",
    "import json\n",
    "\n",
    "with open('Dictionary2.json', 'w') as fp:\n",
    "    json.dump(dictionar3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionar3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I import the files I need and prepare the code for the search engine2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data) #first dict with every word and a unique value\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)#first inverted dict with number and list of document with a word that has that number in diction\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\ncontain.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "ncontain = json.loads(data) #dict with number of times a word appear in all the cod\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\idfdict.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "idfdict = json.loads(data) #dict with the idf for every word\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary2.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction3 = json.loads(data) #second inverted dictionary wit doc and tfidf for eevry word\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASbout copsine similarity I have to take the tfidf of my k best document and multipèly singularly it with the values of the quiery that are tf(relative of the quiery)*idf(relative to my documents for each component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The query entered by the user can also be represented as a vector. We will calculate the TF*IDF for the query\n",
    "\n",
    "TF\tIDF\tTF*IDF\n",
    "life\t0.5\t1.405507153\t0.702753576\n",
    "learning\t0.5\t1.405507153\t0.702753576\n",
    "Let us now calculate the cosine similarity of the query and Document1. You can do the calculation using this tool.\n",
    "\n",
    "Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "Dot product(Query, Document1) \n",
    "     = ((0.702753576) * (0.140550715) + (0.702753576)*(0.140550715))\n",
    "     = 0.197545035151\n",
    "\n",
    "||Query|| = sqrt((0.702753576)2 + (0.702753576)2) = 0.993843638185\n",
    "\n",
    "||Document1|| = sqrt((0.140550715)2 + (0.140550715)2) = 0.198768727354\n",
    "\n",
    "Cosine Similarity(Query, Document) = 0.197545035151 / (0.993843638185) * (0.198768727354)\n",
    "                                        = 0.197545035151 / 0.197545035151\n",
    "                                        = 1\n",
    "        \n",
    "https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prepare the code for the search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are the functions to easily compute the cosine similarity given two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pyplot\n",
    "from scipy.spatial.distance import cosine\n",
    "def simple_dot(a, b):\n",
    "    dsum = 0.\n",
    "    for ((idx,), val) in np.ndenumerate(a):\n",
    "        dsum += float(val) * float(b[idx])\n",
    "    return dsum\n",
    "def l2_norm(a):\n",
    "    return math.sqrt(np.dot(a, a))\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b) / (l2_norm(a)* l2_norm(b))\n",
    "np.dot([1,2],[3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remind that Docume_and_words is a list with inside every intro-plot(preprocessed) for every document, so I can just accees to it for my research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENGINE2!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I build the search engine2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WESTERN 1980 film hollywood\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stir Crazy (film)</td>\n",
       "      <td>Stir Crazy               is a 1980 Am...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stir_Crazy_(film)</td>\n",
       "      <td>0.9688606777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heaven's Gate (film)</td>\n",
       "      <td>Heaven's Gate               is a 1980...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Heaven%27s_Gate_...</td>\n",
       "      <td>0.9436166444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Title                                              Intro  \\\n",
       "0     Stir Crazy (film)           Stir Crazy               is a 1980 Am...   \n",
       "1  Heaven's Gate (film)           Heaven's Gate               is a 1980...   \n",
       "\n",
       "                                                 Url    Similarity  \n",
       "0    https://en.wikipedia.org/wiki/Stir_Crazy_(film)  0.9688606777  \n",
       "1  https://en.wikipedia.org/wiki/Heaven%27s_Gate_...  0.9436166444  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HERE i DEFINE THE FUCTION TO RECEIVE THE DATASET, GIVEN AN INPUT\n",
    "import heapq as hq\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "y = list(input().split())\n",
    "def searchengine2(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictiionary based on the input\n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        #print(yfinal)\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "#print(starting_values)\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "        #print(codes)\n",
    "            new = []\n",
    "            for film in final_values:\n",
    "                #print(film)\n",
    "                if film in diction2[yfinal[codes]]:\n",
    "                    new.append(film)\n",
    "            final_values = new\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url', 'Similarity'])\n",
    "    #megaDataframe\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:  \n",
    "            lstofl = []\n",
    "            #here there is a lstofl that has vectors associated with every document, in order of final_values\n",
    "            for film in final_values:\n",
    "                item = []\n",
    "                for code in yfinal:\n",
    "                    for value in diction3[code]:\n",
    "                        if film in value:\n",
    "                            item.append(value[1])\n",
    "                            break\n",
    "                lstofl.append(item)\n",
    "           # print(lstofl) \n",
    "            #Now I have to create the inquiry vector and get the cosine similarity of beetween it and every component of lstofl \n",
    "            query = []\n",
    "            for i in y:\n",
    "                #print(idfdict[i])\n",
    "                query.append(tf(i,y)*idfdict[i])\n",
    "            #print(query)\n",
    "            cossim = []\n",
    "            for vector in lstofl:\n",
    "                cossim.append(cosine_similarity(query, vector))\n",
    "            #print(cossim) #the cosine similariotyb in order of apparition of my document\n",
    "            dict_sim = {}\n",
    "            for indx in range(len(cossim)):\n",
    "                sim = cossim[indx]\n",
    "                if sim not in dict_sim:\n",
    "                    #print(sim)\n",
    "                    dict_sim[sim]=[final_values[indx]]\n",
    "                    #print(dict_sim[sim])\n",
    "                else:\n",
    "                    #print( type(dict_sim))\n",
    "                    dict_sim[sim].append(final_values[indx])\n",
    "                    #print(dict_sim[sim])\n",
    "            #print(dict_sim[1.0])\n",
    "            Peak = 20\n",
    "            #HERE THE HEAP ALGORITHM\n",
    "            to_select = hq.nlargest(Peak, cossim)   \n",
    "            #print(to_select)\n",
    "            k=0\n",
    "            #Now i have the name key(cossim) and values(docum) and I have to take the first 15 of them.\n",
    "            for i in to_select:\n",
    "                if(k<Peak):\n",
    "                    for document in dict_sim[i]:\n",
    "                            if(k>Peak):\n",
    "                                return megaDataframe\n",
    "                            totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                            totakeurl = str(int(totakeurl.replace('.tsv', '')))\n",
    "                            url = dicturls[totakeurl]\n",
    "                            Similarity = format(i, '.10g')\n",
    "                            #print(i)\n",
    "                            temporary = pd.read_csv('Tsvfiles/'+'film'+(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                            title = temporary['title'][0]\n",
    "                            intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                            new_row = [title, intro, url, Similarity]\n",
    "                            megaDataframe.loc[k]=new_row\n",
    "                            k=k+1\n",
    "            return  megaDataframe\n",
    "A = searchengine2(y)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "live long life 1980\n",
      "[1.0000000000000002, 1.0, 1.0, 1.0, 1.0, 0.9970165961462043, 0.9759146912636064, 0.9759146912636063, 0.9759146912636063, 0.9759146912636063, 0.9759146912636063, 0.9759146912636063, 0.9700114308450829, 0.9693133984648382, 0.9632525818744059, 0.9592192822291048, 0.9592192822291046, 0.9489797547495359, 0.9209133015472398, 0.9209133015472396]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Company You Keep (film)</td>\n",
       "      <td>The Company You Keep               is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Company_You_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maximum Overdrive</td>\n",
       "      <td>Maximum Overdrive               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Maximum_Overdrive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kabul Express</td>\n",
       "      <td>Kabul Express               (        ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kabul_Express</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ore Mukham</td>\n",
       "      <td>Ore Mukham               (           ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ore_Mugham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doorathu Idi Muzhakkam</td>\n",
       "      <td>Doorathu Idi Muzhakkam               ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Doorathu_Idi_Muz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Maximum Overdrive</td>\n",
       "      <td>Maximum Overdrive               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Maximum_Overdrive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kabul Express</td>\n",
       "      <td>Kabul Express               (        ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kabul_Express</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ore Mukham</td>\n",
       "      <td>Ore Mukham               (           ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ore_Mugham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Doorathu Idi Muzhakkam</td>\n",
       "      <td>Doorathu Idi Muzhakkam               ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Doorathu_Idi_Muz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Maximum Overdrive</td>\n",
       "      <td>Maximum Overdrive               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Maximum_Overdrive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kabul Express</td>\n",
       "      <td>Kabul Express               (        ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kabul_Express</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ore Mukham</td>\n",
       "      <td>Ore Mukham               (           ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ore_Mugham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Doorathu Idi Muzhakkam</td>\n",
       "      <td>Doorathu Idi Muzhakkam               ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Doorathu_Idi_Muz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Maximum Overdrive</td>\n",
       "      <td>Maximum Overdrive               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Maximum_Overdrive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kabul Express</td>\n",
       "      <td>Kabul Express               (        ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kabul_Express</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ore Mukham</td>\n",
       "      <td>Ore Mukham               (           ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ore_Mugham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Doorathu Idi Muzhakkam</td>\n",
       "      <td>Doorathu Idi Muzhakkam               ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Doorathu_Idi_Muz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kudumbam Oru Kadambam</td>\n",
       "      <td>Kudumbam Oru Kadambam                ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kudumbam_Oru_Kad...</td>\n",
       "      <td>0.9970165961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Tintorera</td>\n",
       "      <td>Tintorera               is a 1977 Mex...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Tintorera</td>\n",
       "      <td>0.9759146913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Space Raiders (film)</td>\n",
       "      <td>Space Raiders               , also kn...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Space_Raiders_(f...</td>\n",
       "      <td>0.9759146913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Videodrome</td>\n",
       "      <td>Videodrome               is a 1983 Ca...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Videodrome</td>\n",
       "      <td>0.9759146913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Title  \\\n",
       "0   The Company You Keep (film)   \n",
       "1             Maximum Overdrive   \n",
       "2                 Kabul Express   \n",
       "3                    Ore Mukham   \n",
       "4        Doorathu Idi Muzhakkam   \n",
       "5             Maximum Overdrive   \n",
       "6                 Kabul Express   \n",
       "7                    Ore Mukham   \n",
       "8        Doorathu Idi Muzhakkam   \n",
       "9             Maximum Overdrive   \n",
       "10                Kabul Express   \n",
       "11                   Ore Mukham   \n",
       "12       Doorathu Idi Muzhakkam   \n",
       "13            Maximum Overdrive   \n",
       "14                Kabul Express   \n",
       "15                   Ore Mukham   \n",
       "16       Doorathu Idi Muzhakkam   \n",
       "17        Kudumbam Oru Kadambam   \n",
       "18                    Tintorera   \n",
       "19         Space Raiders (film)   \n",
       "20                   Videodrome   \n",
       "\n",
       "                                                Intro  \\\n",
       "0            The Company You Keep               is...   \n",
       "1            Maximum Overdrive               is a ...   \n",
       "2            Kabul Express               (        ...   \n",
       "3            Ore Mukham               (           ...   \n",
       "4            Doorathu Idi Muzhakkam               ...   \n",
       "5            Maximum Overdrive               is a ...   \n",
       "6            Kabul Express               (        ...   \n",
       "7            Ore Mukham               (           ...   \n",
       "8            Doorathu Idi Muzhakkam               ...   \n",
       "9            Maximum Overdrive               is a ...   \n",
       "10           Kabul Express               (        ...   \n",
       "11           Ore Mukham               (           ...   \n",
       "12           Doorathu Idi Muzhakkam               ...   \n",
       "13           Maximum Overdrive               is a ...   \n",
       "14           Kabul Express               (        ...   \n",
       "15           Ore Mukham               (           ...   \n",
       "16           Doorathu Idi Muzhakkam               ...   \n",
       "17           Kudumbam Oru Kadambam                ...   \n",
       "18           Tintorera               is a 1977 Mex...   \n",
       "19           Space Raiders               , also kn...   \n",
       "20           Videodrome               is a 1983 Ca...   \n",
       "\n",
       "                                                  Url    Similarity  \n",
       "0   https://en.wikipedia.org/wiki/The_Company_You_...             1  \n",
       "1     https://en.wikipedia.org/wiki/Maximum_Overdrive             1  \n",
       "2         https://en.wikipedia.org/wiki/Kabul_Express             1  \n",
       "3            https://en.wikipedia.org/wiki/Ore_Mugham             1  \n",
       "4   https://en.wikipedia.org/wiki/Doorathu_Idi_Muz...             1  \n",
       "5     https://en.wikipedia.org/wiki/Maximum_Overdrive             1  \n",
       "6         https://en.wikipedia.org/wiki/Kabul_Express             1  \n",
       "7            https://en.wikipedia.org/wiki/Ore_Mugham             1  \n",
       "8   https://en.wikipedia.org/wiki/Doorathu_Idi_Muz...             1  \n",
       "9     https://en.wikipedia.org/wiki/Maximum_Overdrive             1  \n",
       "10        https://en.wikipedia.org/wiki/Kabul_Express             1  \n",
       "11           https://en.wikipedia.org/wiki/Ore_Mugham             1  \n",
       "12  https://en.wikipedia.org/wiki/Doorathu_Idi_Muz...             1  \n",
       "13    https://en.wikipedia.org/wiki/Maximum_Overdrive             1  \n",
       "14        https://en.wikipedia.org/wiki/Kabul_Express             1  \n",
       "15           https://en.wikipedia.org/wiki/Ore_Mugham             1  \n",
       "16  https://en.wikipedia.org/wiki/Doorathu_Idi_Muz...             1  \n",
       "17  https://en.wikipedia.org/wiki/Kudumbam_Oru_Kad...  0.9970165961  \n",
       "18            https://en.wikipedia.org/wiki/Tintorera  0.9759146913  \n",
       "19  https://en.wikipedia.org/wiki/Space_Raiders_(f...  0.9759146913  \n",
       "20           https://en.wikipedia.org/wiki/Videodrome  0.9759146913  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batman sleep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inception</td>\n",
       "      <td>Inception               is a 2010    ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Inception</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arthur (2011 film)</td>\n",
       "      <td>Arthur               is a 2011 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Arthur_(2011_film)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inception</td>\n",
       "      <td>Inception               is a 2010    ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Inception</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arthur (2011 film)</td>\n",
       "      <td>Arthur               is a 2011 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Arthur_(2011_film)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murder on the Orient Express (1974 film)</td>\n",
       "      <td>Murder on the Orient Express         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Murder_on_the_Or...</td>\n",
       "      <td>0.98560670156585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0                                 Inception   \n",
       "1                        Arthur (2011 film)   \n",
       "2                                 Inception   \n",
       "3                        Arthur (2011 film)   \n",
       "4  Murder on the Orient Express (1974 film)   \n",
       "\n",
       "                                               Intro  \\\n",
       "0           Inception               is a 2010    ...   \n",
       "1           Arthur               is a 2011 Americ...   \n",
       "2           Inception               is a 2010    ...   \n",
       "3           Arthur               is a 2011 Americ...   \n",
       "4           Murder on the Orient Express         ...   \n",
       "\n",
       "                                                 Url        Similarity  \n",
       "0            https://en.wikipedia.org/wiki/Inception                 1  \n",
       "1   https://en.wikipedia.org/wiki/Arthur_(2011_film)                 1  \n",
       "2            https://en.wikipedia.org/wiki/Inception                 1  \n",
       "3   https://en.wikipedia.org/wiki/Arthur_(2011_film)                 1  \n",
       "4  https://en.wikipedia.org/wiki/Murder_on_the_Or...  0.98560670156585  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know light film star\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adventures of Captain Marvel</td>\n",
       "      <td>Adventures of Captain Marvel         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adventures_of_Ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hour Before the Dawn</td>\n",
       "      <td>The Hour Before the Dawn             ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Hour_Before_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Journey into Light</td>\n",
       "      <td>Journey into Light               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Journey_into_Light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adventures of Captain Marvel</td>\n",
       "      <td>Adventures of Captain Marvel         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adventures_of_Ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hour Before the Dawn</td>\n",
       "      <td>The Hour Before the Dawn             ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Hour_Before_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Journey into Light</td>\n",
       "      <td>Journey into Light               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Journey_into_Light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adventures of Captain Marvel</td>\n",
       "      <td>Adventures of Captain Marvel         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adventures_of_Ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Hour Before the Dawn</td>\n",
       "      <td>The Hour Before the Dawn             ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Hour_Before_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Journey into Light</td>\n",
       "      <td>Journey into Light               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Journey_into_Light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Surviving the Game</td>\n",
       "      <td>Surviving the Game               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Surviving_the_Game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fools Rush In (1997 film)</td>\n",
       "      <td>Fools Rush In               is a 1997...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Fools_Rush_In_(1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Night of the Demons 3</td>\n",
       "      <td>Night of the Demons 3               (...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Night_of_the_Dem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stir of Echoes</td>\n",
       "      <td>Stir of Echoes               is a 199...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stir_of_Echoes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Corky Romano</td>\n",
       "      <td>Corky Romano               is a 2001 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Corky_Romano</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stolen (2012 film)</td>\n",
       "      <td>Stolen               , formerly known...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stolen_(2012_film)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>From Hell to Texas</td>\n",
       "      <td>From Hell to Texas               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/From_Hell_to_Texas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Silent Running</td>\n",
       "      <td>Silent Running               is a 197...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Silent_Running</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A Dark Song</td>\n",
       "      <td>A Dark Song               is a 2016 I...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A_Dark_Song</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Surviving the Game</td>\n",
       "      <td>Surviving the Game               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Surviving_the_Game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fools Rush In (1997 film)</td>\n",
       "      <td>Fools Rush In               is a 1997...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Fools_Rush_In_(1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Night of the Demons 3</td>\n",
       "      <td>Night of the Demons 3               (...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Night_of_the_Dem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title  \\\n",
       "0   Adventures of Captain Marvel   \n",
       "1       The Hour Before the Dawn   \n",
       "2             Journey into Light   \n",
       "3   Adventures of Captain Marvel   \n",
       "4       The Hour Before the Dawn   \n",
       "5             Journey into Light   \n",
       "6   Adventures of Captain Marvel   \n",
       "7       The Hour Before the Dawn   \n",
       "8             Journey into Light   \n",
       "9             Surviving the Game   \n",
       "10     Fools Rush In (1997 film)   \n",
       "11         Night of the Demons 3   \n",
       "12                Stir of Echoes   \n",
       "13                  Corky Romano   \n",
       "14            Stolen (2012 film)   \n",
       "15            From Hell to Texas   \n",
       "16                Silent Running   \n",
       "17                   A Dark Song   \n",
       "18            Surviving the Game   \n",
       "19     Fools Rush In (1997 film)   \n",
       "20         Night of the Demons 3   \n",
       "\n",
       "                                                Intro  \\\n",
       "0            Adventures of Captain Marvel         ...   \n",
       "1            The Hour Before the Dawn             ...   \n",
       "2            Journey into Light               is a...   \n",
       "3            Adventures of Captain Marvel         ...   \n",
       "4            The Hour Before the Dawn             ...   \n",
       "5            Journey into Light               is a...   \n",
       "6            Adventures of Captain Marvel         ...   \n",
       "7            The Hour Before the Dawn             ...   \n",
       "8            Journey into Light               is a...   \n",
       "9            Surviving the Game               is a...   \n",
       "10           Fools Rush In               is a 1997...   \n",
       "11           Night of the Demons 3               (...   \n",
       "12           Stir of Echoes               is a 199...   \n",
       "13           Corky Romano               is a 2001 ...   \n",
       "14           Stolen               , formerly known...   \n",
       "15           From Hell to Texas               is a...   \n",
       "16           Silent Running               is a 197...   \n",
       "17           A Dark Song               is a 2016 I...   \n",
       "18           Surviving the Game               is a...   \n",
       "19           Fools Rush In               is a 1997...   \n",
       "20           Night of the Demons 3               (...   \n",
       "\n",
       "                                                  Url Similarity  \n",
       "0   https://en.wikipedia.org/wiki/Adventures_of_Ca...          1  \n",
       "1   https://en.wikipedia.org/wiki/The_Hour_Before_...          1  \n",
       "2    https://en.wikipedia.org/wiki/Journey_into_Light          1  \n",
       "3   https://en.wikipedia.org/wiki/Adventures_of_Ca...          1  \n",
       "4   https://en.wikipedia.org/wiki/The_Hour_Before_...          1  \n",
       "5    https://en.wikipedia.org/wiki/Journey_into_Light          1  \n",
       "6   https://en.wikipedia.org/wiki/Adventures_of_Ca...          1  \n",
       "7   https://en.wikipedia.org/wiki/The_Hour_Before_...          1  \n",
       "8    https://en.wikipedia.org/wiki/Journey_into_Light          1  \n",
       "9    https://en.wikipedia.org/wiki/Surviving_the_Game          1  \n",
       "10  https://en.wikipedia.org/wiki/Fools_Rush_In_(1...          1  \n",
       "11  https://en.wikipedia.org/wiki/Night_of_the_Dem...          1  \n",
       "12       https://en.wikipedia.org/wiki/Stir_of_Echoes          1  \n",
       "13         https://en.wikipedia.org/wiki/Corky_Romano          1  \n",
       "14   https://en.wikipedia.org/wiki/Stolen_(2012_film)          1  \n",
       "15   https://en.wikipedia.org/wiki/From_Hell_to_Texas          1  \n",
       "16       https://en.wikipedia.org/wiki/Silent_Running          1  \n",
       "17          https://en.wikipedia.org/wiki/A_Dark_Song          1  \n",
       "18   https://en.wikipedia.org/wiki/Surviving_the_Game          1  \n",
       "19  https://en.wikipedia.org/wiki/Fools_Rush_In_(1...          1  \n",
       "20  https://en.wikipedia.org/wiki/Night_of_the_Dem...          1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disney movie 2019\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thumbelina (1994 film)</td>\n",
       "      <td>Thumbelina               (also known ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Avengers (2012 film)</td>\n",
       "      <td>Marvel's The Avengers                ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>Cars 3               is a 2017 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "      <td>0.94335701356643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frozen (2013 film)</td>\n",
       "      <td>Frozen               is a 2013 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "      <td>0.89654294215813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>The Chronicles of Narnia: Prince Casp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "      <td>0.82684193281763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0                    Thumbelina (1994 film)   \n",
       "1                  The Avengers (2012 film)   \n",
       "2                                    Cars 3   \n",
       "3                        Frozen (2013 film)   \n",
       "4  The Chronicles of Narnia: Prince Caspian   \n",
       "\n",
       "                                               Intro  \\\n",
       "0           Thumbelina               (also known ...   \n",
       "1           Marvel's The Avengers                ...   \n",
       "2           Cars 3               is a 2017 Americ...   \n",
       "3           Frozen               is a 2013 Americ...   \n",
       "4           The Chronicles of Narnia: Prince Casp...   \n",
       "\n",
       "                                                 Url        Similarity  \n",
       "0  https://en.wikipedia.org/wiki/Thumbelina_(1994...                 1  \n",
       "1  https://en.wikipedia.org/wiki/The_Avengers_(20...                 1  \n",
       "2               https://en.wikipedia.org/wiki/Cars_3  0.94335701356643  \n",
       "3   https://en.wikipedia.org/wiki/Frozen_(2013_film)  0.89654294215813  \n",
       "4  https://en.wikipedia.org/wiki/The_Chronicles_o...  0.82684193281763  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex[3] build a new engine with a personal Score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
