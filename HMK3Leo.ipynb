{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.1] Get the list of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/10_to_Midnight'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies2.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "urls = []\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies1.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "lst_a\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[10000]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Z.P.G.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies3.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "lst_a\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[20000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Whistle_(2003_film)'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[29999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicturls = {}\n",
    "for i in range(len(urls)):\n",
    "    dicturls[i] = urls[i]\n",
    "with open('dicturls.json', 'w') as fp:\n",
    "    json.dump(dicturls, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Whistle_(2003_film)'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicturls[str(29999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.2] Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "import time\n",
    "def getwikipageshtml(urls):\n",
    "    k=0\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            ur_l = requests.get(urls[i])\n",
    "            soup = BeautifulSoup(ur_l.content, 'html.parser')\n",
    "            soup = soup.prettify(\"utf-8\")   \n",
    "            stringa = 'Articles/article-'+str(k)+'.html'\n",
    "            k = k+1\n",
    "            Html_file= open(stringa, \"wb\")\n",
    "            Html_file.write(soup)\n",
    "            Html_file.close()\n",
    "        except(URLError,HTTPError, ContentTooShortError)  as e:\n",
    "            html = None\n",
    "        #time.sleep(0.001) #Actually for this task we don't need to stop anytime\n",
    "    return\n",
    "getwikipageshtml(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.3] Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open(\"Articles/article-0.html\")\n",
    "#soup = BeautifulSoup(myfile, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "for i in range(30000):\n",
    "    filename = \"Articles/article-\"+str(i)+\".html\"\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        #Now That I opened the file I have to look for the section asked\n",
    "        #first I take the title and clear him of spaces and the word -Wikipedia\n",
    "        titlepage = soup.title.string\n",
    "        titleonly = titlepage.split(\"- Wikipedia\")\n",
    "        titlepage = titleonly[0].strip()\n",
    "        #now I create empty string as intro and plot\n",
    "        intro = ''\n",
    "        plot = ''\n",
    "        #Now i searcvh the first paragraph that usually or is empty or is the intro\n",
    "        start = soup.find('p')\n",
    "        intro = start\n",
    "        intro1 = start.text\n",
    "        B = ''\n",
    "        #in B I put m,y limit for the paragraphs in the intro, because after this h2 there will always be the plot\n",
    "        B = intro.find_next_sibling('h2')\n",
    "        if(B!=None and B.find_next_sibling('p')):\n",
    "            C = B.find_next_sibling('p')\n",
    "            while(C != intro.find_next_sibling('p')): \n",
    "                intro1 = intro1 + intro.find_next_sibling('p').text\n",
    "                intro = intro.find_next_sibling('p')\n",
    "            plot = ''    \n",
    "            #then i do the same with the plot, so I start at B and end in the next h2\n",
    "            plot = B\n",
    "            plot1 = ''\n",
    "            compare = ''\n",
    "            if(B.find_next_sibling('h2')):\n",
    "                compare = B.find_next_sibling('h2')\n",
    "                compareto = compare.find_next_sibling('p')\n",
    "                while(compareto != plot.find_next_sibling('p')):\n",
    "                    plot1 = plot1 + plot.find_next_sibling('p').text\n",
    "                    #print(plot1)\n",
    "                    plot = plot.find_next_sibling('p')\n",
    "                    #if plot or intro are empty I put NA\n",
    "        if(intro1 == ''):\n",
    "            intro1 = \"NA\"\n",
    "        if plot1 == '':\n",
    "            plot1 = \"NA\"\n",
    "        #Now I start working on the other features\n",
    "        intro = intro1\n",
    "        plot = plot1\n",
    "        film_name = 'NA'\n",
    "        director = \"NA\"\n",
    "        producer = \"NA\"\n",
    "        writer = \"NA\"\n",
    "        starring = \"NA\"\n",
    "        music = \"NA\"\n",
    "        release_date = \"NA\"\n",
    "        runtime = \"NA\"\n",
    "        country = \"NA\"\n",
    "        language = \"NA\"\n",
    "        budget = \"NA\"\n",
    "#'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'\n",
    "        for link in soup.find_all('tr'):\n",
    "            if soup.find('th',{'class': ['summary']})!= None:\n",
    "                    film_name = soup.find('th',{'class': ['summary']} ).text.strip()\n",
    "            if link.th:\n",
    "#I just check in the th and if I find the class I need I take the relative td. Some of them are inaccessible so the if link.td\n",
    "                if(link.th.text.strip() == 'Directed by'):\n",
    "                     director = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Produced by'):\n",
    "                      producer = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Written by'):\n",
    "                    writer = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Starring'):\n",
    "                    starring = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Music by'):\n",
    "                     music = link.td.text.strip()               \n",
    "                elif(link.th.text.strip() == 'Release date'):\n",
    "                    if link.td:\n",
    "                        release_date = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Running time'):\n",
    "                    runtime = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Country'):\n",
    "                    if link.td:\n",
    "                        country = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Language'):\n",
    "                    if link.td:\n",
    "                        language = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Budget'):\n",
    "                    budget = link.td.text.strip()\n",
    "#now I open the tsv files and create one for every film.        \n",
    "        tsvname = 'Tsvfiles/'+'film'+str(i)+'.tsv'\n",
    "        with io.open(tsvname, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['title','intro', 'plot', 'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'])\n",
    "            tsv_writer.writerow([titlepage, intro, plot, film_name, director, producer, writer, starring, music, release_date, runtime, \n",
    "                 country, language, budget])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whistle (2003 film)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n         What Became of Jack and Jill?\\n        \\n\\n       is a 1972 British\\n       \\n        horror film\\n       \\n       directed by\\n       \\n        Bill Bain\\n       \\n       and starring\\n       \\n        Mona Washbourne\\n       \\n       ,\\n       \\n        Paul Nicholas\\n       \\n       , and\\n       \\n        Vanessa Howard\\n       \\n       .\\n       \\n\\n         [1]\\n        \\n\\n       It was part of an abandoned attempt by\\n       \\n        Amicus Pictures\\n       \\n       to compete with\\n       \\n        Hammer Studios\\n       \\n       by breaking into the\\n       \\n        grindhouse\\n       \\n       market. Studio executives were ultimately too disturbed by the final product to release it under the Amicus name, and they sold the film to\\n       \\n        20th Century Fox\\n       \\n       .\\n      '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is just to try stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\leona\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\leona\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>plot</th>\n",
       "      <th>film_name</th>\n",
       "      <th>director</th>\n",
       "      <th>producer</th>\n",
       "      <th>writer</th>\n",
       "      <th>starring</th>\n",
       "      <th>music</th>\n",
       "      <th>release date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>country</th>\n",
       "      <th>language</th>\n",
       "      <th>budget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>big chill film</td>\n",
       "      <td>big chill 1983 american comedi drama film dire...</td>\n",
       "      <td>harold cooper bath young son wife dr sarah coo...</td>\n",
       "      <td>big chill</td>\n",
       "      <td>lawrenc kasdan</td>\n",
       "      <td>michael shamberg</td>\n",
       "      <td>lawrenc kasdan barbara benedek</td>\n",
       "      <td>tom bereng glenn close jeff goldblum william h...</td>\n",
       "      <td>bill conti</td>\n",
       "      <td>septemb 28 1983 1983 09 28</td>\n",
       "      <td>105 minut</td>\n",
       "      <td>unit state</td>\n",
       "      <td>english</td>\n",
       "      <td>8 million 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            title                                              intro  \\\n",
       "0  big chill film  big chill 1983 american comedi drama film dire...   \n",
       "\n",
       "                                                plot  film_name  \\\n",
       "0  harold cooper bath young son wife dr sarah coo...  big chill   \n",
       "\n",
       "         director          producer                          writer  \\\n",
       "0  lawrenc kasdan  michael shamberg  lawrenc kasdan barbara benedek   \n",
       "\n",
       "                                            starring       music  \\\n",
       "0  tom bereng glenn close jeff goldblum william h...  bill conti   \n",
       "\n",
       "                 release date    runtime     country language       budget  \n",
       "0  septemb 28 1983 1983 09 28  105 minut  unit state  english  8 million 1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset1 = pd.read_csv('Cleantsv/filmclean-8.tsv', delimiter='\\t')\n",
    "#dataset1 = pd.read_csv('Tsvfiles/film1', delimiter='\\t')\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'budget 10 midnight'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I clean The files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "#I use the function on every section of the tsv files, except for the tiles of the categories, so from the part 13\n",
    "for i in range(30000):\n",
    "    file1 = open('Tsvfiles/film'+str(i)+'.tsv', encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    words = line.split('\\t') \n",
    "    for j in range(13, len(words)):\n",
    "        words[j] = preprocess(words[j])\n",
    "        if j ==13:\n",
    "            #Here for the format of tsv files and my split('\\t') the word budget would always be in my title, so I take her out\n",
    "            words[j] = words[j].replace('budget ','')    \n",
    "    tsvname = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    with io.open(tsvname, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['title','intro', 'plot', 'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'])\n",
    "            tsv_writer.writerow(words[13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whistl 2003 film'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOw I create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionar = {}\n",
    "k = 0\n",
    "for i in range(30000):\n",
    "    file1 = open(\"Cleantsv/filmclean-\"+str(i)+'.tsv', encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    #print(wordssplitted1, wordssplitted2)\n",
    "    for i in wordssplitted1:\n",
    "        #print(type(i))\n",
    "        if i not in dictionar:\n",
    "            dictionar[i] = str(k)\n",
    "            k = k+1\n",
    "    for i in wordssplitted2:\n",
    "        #print(type(i))\n",
    "        if i not in dictionar:\n",
    "            dictionar[i] = str(k)\n",
    "            k = k+1\n",
    "#dictionar\n",
    "import json\n",
    "\n",
    "with open('Dictionary.json', 'w') as fp:\n",
    "    json.dump(dictionar, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12741'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data)\n",
    "#dictions = pd.DataFrame(diction)\n",
    "diction['1913']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diction)\n",
    "type(diction['hom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the inverted Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionar2 = {}\n",
    "length = 0\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    #print(wordssplitted1, wordssplitted2)\n",
    "    for j in wordssplitted1:\n",
    "        code = diction[j]\n",
    "        if code not in dictionar2:\n",
    "            dictionar2[code] = [file]\n",
    "        elif file not in dictionar2[code]:\n",
    "            dictionar2[code].append(file)\n",
    "    for j in wordssplitted2:\n",
    "        code = diction[j]\n",
    "        if code not in dictionar2:\n",
    "            dictionar2[code] = [file]\n",
    "        elif file not in dictionar2[code]:\n",
    "            dictionar2[code].append(file)\n",
    "#dictionar\n",
    "import json\n",
    "\n",
    "with open('Dictionary1.json', 'w') as fp:\n",
    "    json.dump(dictionar2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)\n",
    "#dictions = pd.DataFrame(diction)\n",
    "#diction2['0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diction['2019']\n",
    "len(diction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#diction2['7507']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First engine searchengine1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ross\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Superman III</td>\n",
       "      <td>Superman III               is a Briti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Superman_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Footloose (1984 film)</td>\n",
       "      <td>Footloose               is a 1984 Ame...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Footloose_(1984_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Protocol (film)</td>\n",
       "      <td>Protocol               is a 1984 Amer...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Protocol_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Splash (film)</td>\n",
       "      <td>Splash               is a 1984 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Splash_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday the 13th: A New Beginning</td>\n",
       "      <td>Friday the 13th: A New Beginning     ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Friday_the_13th:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Turning Paige</td>\n",
       "      <td>Turning Paige               is a 2001...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Turning_Paige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>The Saddest Music in the World</td>\n",
       "      <td>The Saddest Music in the World       ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Saddest_Musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Goon (film)</td>\n",
       "      <td>Goon               is a 2011 Canadian...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Goon_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Charming (film)</td>\n",
       "      <td>Charming               is a 2018 Cana...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Charming_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Goon: Last of the Enforcers</td>\n",
       "      <td>Goon: Last of the Enforcers          ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Goon:_Last_of_th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title  \\\n",
       "0                        Superman III   \n",
       "1               Footloose (1984 film)   \n",
       "2                     Protocol (film)   \n",
       "3                       Splash (film)   \n",
       "4    Friday the 13th: A New Beginning   \n",
       "..                                ...   \n",
       "351                     Turning Paige   \n",
       "352    The Saddest Music in the World   \n",
       "353                       Goon (film)   \n",
       "354                   Charming (film)   \n",
       "355       Goon: Last of the Enforcers   \n",
       "\n",
       "                                                 Intro  \\\n",
       "0             Superman III               is a Briti...   \n",
       "1             Footloose               is a 1984 Ame...   \n",
       "2             Protocol               is a 1984 Amer...   \n",
       "3             Splash               is a 1984 Americ...   \n",
       "4             Friday the 13th: A New Beginning     ...   \n",
       "..                                                 ...   \n",
       "351           Turning Paige               is a 2001...   \n",
       "352           The Saddest Music in the World       ...   \n",
       "353           Goon               is a 2011 Canadian...   \n",
       "354           Charming               is a 2018 Cana...   \n",
       "355           Goon: Last of the Enforcers          ...   \n",
       "\n",
       "                                                   Url  \n",
       "0           https://en.wikipedia.org/wiki/Superman_III  \n",
       "1    https://en.wikipedia.org/wiki/Footloose_(1984_...  \n",
       "2        https://en.wikipedia.org/wiki/Protocol_(film)  \n",
       "3          https://en.wikipedia.org/wiki/Splash_(film)  \n",
       "4    https://en.wikipedia.org/wiki/Friday_the_13th:...  \n",
       "..                                                 ...  \n",
       "351        https://en.wikipedia.org/wiki/Turning_Paige  \n",
       "352  https://en.wikipedia.org/wiki/The_Saddest_Musi...  \n",
       "353          https://en.wikipedia.org/wiki/Goon_(film)  \n",
       "354      https://en.wikipedia.org/wiki/Charming_(film)  \n",
       "355  https://en.wikipedia.org/wiki/Goon:_Last_of_th...  \n",
       "\n",
       "[356 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HERE i DEFINE THE FUCTION TO RECEIVE THE DATASET, GIVEN AN INPUT\n",
    "y = list(input().split())\n",
    "def searchengine1(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictiionary based on the input\n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        #print(yfinal)\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "#print(starting_values)\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "        #print(codes)\n",
    "            new = []\n",
    "            for film in final_values:\n",
    "            #print(film)\n",
    "                if film in diction2[yfinal[codes]]:\n",
    "                    new.append(film)\n",
    "            final_values = new\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url'])\n",
    "    #megaDataframe\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:\n",
    "            k=0\n",
    "            for document in final_values:\n",
    "                totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                totakeurl = str(int(totakeurl.replace('.tsv', '')))\n",
    "                url = dicturls[totakeurl]\n",
    "                temporary = pd.read_csv('Tsvfiles/'+'film'+(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                title = temporary['title'][0]\n",
    "                intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                new_row = [title, intro, url]\n",
    "                megaDataframe.loc[k]=new_row\n",
    "                k=k+1\n",
    "            return megaDataframe\n",
    "A = searchengine1(y)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building of the second search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)\n",
    "#print(Docum_and_words) \n",
    "#I have the lis of documents with intro and plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary with idf to save computational costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary declarations to run the code at any moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 midnight 1983 american crime horror thriller film 3 direct j lee thompson screenplay origin written william robert film star charl bronson lead role support cast includ lisa eilbach andrew steven gene davi geoffrey lewi wilford brimley 10 midnight releas citi film subsidiari cannon film american cinema march 11 1983 warren staci gene davi young offic equip repairman kill women reject sexual advanc attempt flirt alway seen creepi women result frequent reject 4 first victim betti june gilbert offic worker acquaint track wood area observ sex boyfriend ambush coupl kill boyfriend give chase nake woman catch stab death 4 two lo angel polic detect leo kessler charl bronson paul mcann andrew steven investig murder kessler season veteran forc mcann consider younger 4 staci avoid prosecut construct sound alibi assault victim nake except pair latex glove hide fingerprint thu minim evid lauri kessler lisa eilbach daughter leo acquaint victim student nurs becom target killer 4 mcann refus go along kessler plant evid order frame suspect staci goe anoth rampag kill three nurs student friend kessler daughter eventu caught stark nake street staci boast say thing prove crazi hear voic order thing etc one day back street kessler well whole fuck world hear kessler repli shoot staci forehead execut leav consider asid kessler stand bodi surround polic'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Docum_and_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data) #first dict with every word and a unique value\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)#first inverted dict with number and list of document with a word that has that number in diction\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc.split())\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / float(n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncontain = {}\n",
    "for i in diction:\n",
    "    refer = diction[i]\n",
    "    #print(i)\n",
    "    ncontain[i] = len(diction2[refer])\n",
    "with open('ncontain.json', 'w') as fp:\n",
    "    json.dump(ncontain, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\ncontain.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "ncontain = json.loads(data) #first dict with every word and a unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ncontain.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "idfdict = {}\n",
    "for i in ncontain:\n",
    "    idfdict[i]=math.log(30000 / ncontain[i])\n",
    "with open('idfdict.json', 'w') as fp:\n",
    "    json.dump(idfdict, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\idfdict.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "idfdict = json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second dictionary with the tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionar3 = {}\n",
    "#length = 0\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    temp = []\n",
    "    for j in Docum_and_words[i].split():\n",
    "            if j not in temp:\n",
    "                code = diction[j]\n",
    "                value = tf(j, Docum_and_words[i].split())*idfdict[j]\n",
    "                li = [file, value]\n",
    "                if code not in dictionar3:\n",
    "                    dictionar3[code] = [file, value]\n",
    "                else:                \n",
    "                    dictionar3[code].append([file, value])\n",
    "                temp.append(j)\n",
    "import json\n",
    "\n",
    "with open('Dictionary2.json', 'w') as fp:\n",
    "    json.dump(dictionar3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionar3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I import the files I need and prepare the code for the search engine2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data) #first dict with every word and a unique value\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)#first inverted dict with number and list of document with a word that has that number in diction\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\ncontain.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "ncontain = json.loads(data) #dict with number of times a word appear in all the cod\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\idfdict.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "idfdict = json.loads(data) #dict with the idf for every word\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary2.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction3 = json.loads(data) #second inverted dictionary wit doc and tfidf for eevry word\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASbout copsine similarity I have to take the tfidf of my k best document and multipèly singularly it with the values of the quiery that are tf(relative of the quiery)*idf(relative to my documents for each component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The query entered by the user can also be represented as a vector. We will calculate the TF*IDF for the query\n",
    "\n",
    "TF\tIDF\tTF*IDF\n",
    "life\t0.5\t1.405507153\t0.702753576\n",
    "learning\t0.5\t1.405507153\t0.702753576\n",
    "Let us now calculate the cosine similarity of the query and Document1. You can do the calculation using this tool.\n",
    "\n",
    "Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "Dot product(Query, Document1) \n",
    "     = ((0.702753576) * (0.140550715) + (0.702753576)*(0.140550715))\n",
    "     = 0.197545035151\n",
    "\n",
    "||Query|| = sqrt((0.702753576)2 + (0.702753576)2) = 0.993843638185\n",
    "\n",
    "||Document1|| = sqrt((0.140550715)2 + (0.140550715)2) = 0.198768727354\n",
    "\n",
    "Cosine Similarity(Query, Document) = 0.197545035151 / (0.993843638185) * (0.198768727354)\n",
    "                                        = 0.197545035151 / 0.197545035151\n",
    "                                        = 1\n",
    "        \n",
    "https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prepare the code for the search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are the functions to easily compute the cosine similarity given two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pyplot\n",
    "from scipy.spatial.distance import cosine\n",
    "def simple_dot(a, b):\n",
    "    dsum = 0.\n",
    "    for ((idx,), val) in np.ndenumerate(a):\n",
    "        dsum += float(val) * float(b[idx])\n",
    "    return dsum\n",
    "def l2_norm(a):\n",
    "    return math.sqrt(np.dot(a, a))\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b) / (l2_norm(a) * l2_norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remind that Docume_and_words is a list with inside every intro-plot(preprocessed) for every document, so I can just accees to it for my research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I build the search engine2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "y = list(input().split())\n",
    "def searchengine2(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictionary based on the input\n",
    "    #NOw I need the tf for the cosine similarity(the idf is already ina dict)\n",
    "    \n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "            for film in final_values:\n",
    "                if film not in diction2[yfinal[codes]]:\n",
    "                    final_values.remove(film)\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url','Score'])\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:\n",
    "            k=0\n",
    "            for document in final_values:\n",
    "                totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                totakeurl = int(totakeurl.replace('.tsv', ''))\n",
    "                url = urls[totakeurl]\n",
    "                temporary = pd.read_csv('Tsvfiles/'+'film'+str(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                with open(document, encoding=\"utf8\") as file:\n",
    "                    line = file.read()\n",
    "                words = line.split('\\t') \n",
    "                wordssplitted11 = words[14].split()\n",
    "                wordssplitted21 = words[15].split()\n",
    "                s2=str(wordssplitted11)+str(wordssplitted21)\n",
    "                tfidf = TfidfVectorizer()\n",
    "                s1 = diction.keys()\n",
    "                response = tfidf.fit_transform([s2, str(s1)])\n",
    "                feature_names = tfidf.get_feature_names()\n",
    "               # for col in response.nonzero()[1]:\n",
    "                    #print (feature_names[col], ' - ', response[0, col])\n",
    "                title = temporary['title'][0]\n",
    "                intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                new_row = [title, intro, url, feature_names]\n",
    "                megaDataframe.loc[k]=new_row\n",
    "                k=k+1\n",
    "            return (megaDataframe)\n",
    "searchengine2(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
