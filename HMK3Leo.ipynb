{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.1] Get the list of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/10_to_Midnight'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies2.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "urls = []\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies1.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "lst_a\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[10000]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Z.P.G.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies3.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "lst_a\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[20000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Whistle_(2003_film)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[29999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.2] Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "import time\n",
    "def getwikipageshtml(urls):\n",
    "    k=0\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            ur_l = requests.get(urls[i])\n",
    "            soup = BeautifulSoup(ur_l.content, 'html.parser')\n",
    "            soup = soup.prettify(\"utf-8\")   \n",
    "            stringa = 'Articles/article-'+str(k)+'.html'\n",
    "            k = k+1\n",
    "            Html_file= open(stringa, \"wb\")\n",
    "            Html_file.write(soup)\n",
    "            Html_file.close()\n",
    "        except(URLError,HTTPError, ContentTooShortError)  as e:\n",
    "            html = None\n",
    "        #time.sleep(0.001) #Actually for this task we don't need to stop anytime\n",
    "    return\n",
    "getwikipageshtml(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.3] Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open(\"Articles/article-0.html\")\n",
    "#soup = BeautifulSoup(myfile, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "for i in range(30000):\n",
    "    filename = \"Articles/article-\"+str(i)+\".html\"\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        #Now That I opened the file I have to look for the section asked\n",
    "        #first I take the title and clear him of spaces and the word -Wikipedia\n",
    "        titlepage = soup.title.string\n",
    "        titleonly = titlepage.split(\"- Wikipedia\")\n",
    "        titlepage = titleonly[0].strip()\n",
    "        #now I create empty string as intro and plot\n",
    "        intro = ''\n",
    "        plot = ''\n",
    "        #Now i searcvh the first paragraph that usually or is empty or is the intro\n",
    "        start = soup.find('p')\n",
    "        intro = start\n",
    "        intro1 = start.text\n",
    "        B = ''\n",
    "        #in B I put m,y limit for the paragraphs in the intro, because after this h2 there will always be the plot\n",
    "        B = intro.find_next_sibling('h2')\n",
    "        if(B!=None and B.find_next_sibling('p')):\n",
    "            C = B.find_next_sibling('p')\n",
    "            while(C != intro.find_next_sibling('p')): \n",
    "                intro1 = intro1 + intro.find_next_sibling('p').text\n",
    "                intro = intro.find_next_sibling('p')\n",
    "            plot = ''    \n",
    "            #then i do the same with the plot, so I start at B and end in the next h2\n",
    "            plot = B\n",
    "            plot1 = ''\n",
    "            compare = ''\n",
    "            if(B.find_next_sibling('h2')):\n",
    "                compare = B.find_next_sibling('h2')\n",
    "                compareto = compare.find_next_sibling('p')\n",
    "                while(compareto != plot.find_next_sibling('p')):\n",
    "                    plot1 = plot1 + plot.find_next_sibling('p').text\n",
    "                    #print(plot1)\n",
    "                    plot = plot.find_next_sibling('p')\n",
    "                    #if plot or intro are empty I put NA\n",
    "        if(intro1 == ''):\n",
    "            intro1 = \"NA\"\n",
    "        if plot1 == '':\n",
    "            plot1 = \"NA\"\n",
    "        #Now I start working on the other features\n",
    "        intro = intro1\n",
    "        plot = plot1\n",
    "        film_name = 'NA'\n",
    "        director = \"NA\"\n",
    "        producer = \"NA\"\n",
    "        writer = \"NA\"\n",
    "        starring = \"NA\"\n",
    "        music = \"NA\"\n",
    "        release_date = \"NA\"\n",
    "        runtime = \"NA\"\n",
    "        country = \"NA\"\n",
    "        language = \"NA\"\n",
    "        budget = \"NA\"\n",
    "#'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'\n",
    "        for link in soup.find_all('tr'):\n",
    "            if soup.find('th',{'class': ['summary']})!= None:\n",
    "                    film_name = soup.find('th',{'class': ['summary']} ).text.strip()\n",
    "            if link.th:\n",
    "#I just check in the th and if I find the class I need I take the relative td. Some of them are inaccessible so the if link.td\n",
    "                if(link.th.text.strip() == 'Directed by'):\n",
    "                     director = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Produced by'):\n",
    "                      producer = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Written by'):\n",
    "                    writer = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Starring'):\n",
    "                    starring = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Music by'):\n",
    "                     music = link.td.text.strip()               \n",
    "                elif(link.th.text.strip() == 'Release date'):\n",
    "                    if link.td:\n",
    "                        release_date = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Running time'):\n",
    "                    runtime = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Country'):\n",
    "                    if link.td:\n",
    "                        country = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Language'):\n",
    "                    if link.td:\n",
    "                        language = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Budget'):\n",
    "                    budget = link.td.text.strip()\n",
    "#now I open the tsv files and create one for every film.        \n",
    "        tsvname = 'Tsvfiles/'+'film'+str(i)+'.tsv'\n",
    "        with io.open(tsvname, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['title','intro', 'plot', 'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'])\n",
    "            tsv_writer.writerow([titlepage, intro, plot, film_name, director, producer, writer, starring, music, release_date, runtime, \n",
    "                 country, language, budget])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whistle (2003 film)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n         What Became of Jack and Jill?\\n        \\n\\n       is a 1972 British\\n       \\n        horror film\\n       \\n       directed by\\n       \\n        Bill Bain\\n       \\n       and starring\\n       \\n        Mona Washbourne\\n       \\n       ,\\n       \\n        Paul Nicholas\\n       \\n       , and\\n       \\n        Vanessa Howard\\n       \\n       .\\n       \\n\\n         [1]\\n        \\n\\n       It was part of an abandoned attempt by\\n       \\n        Amicus Pictures\\n       \\n       to compete with\\n       \\n        Hammer Studios\\n       \\n       by breaking into the\\n       \\n        grindhouse\\n       \\n       market. Studio executives were ultimately too disturbed by the final product to release it under the Amicus name, and they sold the film to\\n       \\n        20th Century Fox\\n       \\n       .\\n      '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is just to try stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\leona\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\leona\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>plot</th>\n",
       "      <th>film_name</th>\n",
       "      <th>director</th>\n",
       "      <th>producer</th>\n",
       "      <th>writer</th>\n",
       "      <th>starring</th>\n",
       "      <th>music</th>\n",
       "      <th>release date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>country</th>\n",
       "      <th>language</th>\n",
       "      <th>budget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>big chill film</td>\n",
       "      <td>big chill 1983 american comedi drama film dire...</td>\n",
       "      <td>harold cooper bath young son wife dr sarah coo...</td>\n",
       "      <td>big chill</td>\n",
       "      <td>lawrenc kasdan</td>\n",
       "      <td>michael shamberg</td>\n",
       "      <td>lawrenc kasdan barbara benedek</td>\n",
       "      <td>tom bereng glenn close jeff goldblum william h...</td>\n",
       "      <td>bill conti</td>\n",
       "      <td>septemb 28 1983 1983 09 28</td>\n",
       "      <td>105 minut</td>\n",
       "      <td>unit state</td>\n",
       "      <td>english</td>\n",
       "      <td>8 million 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            title                                              intro  \\\n",
       "0  big chill film  big chill 1983 american comedi drama film dire...   \n",
       "\n",
       "                                                plot  film_name  \\\n",
       "0  harold cooper bath young son wife dr sarah coo...  big chill   \n",
       "\n",
       "         director          producer                          writer  \\\n",
       "0  lawrenc kasdan  michael shamberg  lawrenc kasdan barbara benedek   \n",
       "\n",
       "                                            starring       music  \\\n",
       "0  tom bereng glenn close jeff goldblum william h...  bill conti   \n",
       "\n",
       "                 release date    runtime     country language       budget  \n",
       "0  septemb 28 1983 1983 09 28  105 minut  unit state  english  8 million 1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset1 = pd.read_csv('Cleantsv/filmclean-8.tsv', delimiter='\\t')\n",
    "#dataset1 = pd.read_csv('Tsvfiles/film1', delimiter='\\t')\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'budget 10 midnight'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "#I use the function on every section of the tsv files, except for the tiles of the categories, so from the part 13\n",
    "for i in range(30000):\n",
    "    file1 = open('Tsvfiles/film'+str(i)+'.tsv', encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    words = line.split('\\t') \n",
    "    for j in range(13, len(words)):\n",
    "        words[j] = preprocess(words[j])\n",
    "        if j ==13:\n",
    "            #Here for the format of tsv files and my split('\\t') the word budget would always be in my title, so I take her out\n",
    "            words[j] = words[j].replace('budget ','')    \n",
    "    tsvname = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    with io.open(tsvname, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['title','intro', 'plot', 'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'])\n",
    "            tsv_writer.writerow(words[13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whistl 2003 film'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOw I create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionar = {}\n",
    "k = 0\n",
    "for i in range(30000):\n",
    "    file1 = open(\"Cleantsv/filmclean-\"+str(i)+'.tsv', encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    #print(wordssplitted1, wordssplitted2)\n",
    "    for i in wordssplitted1:\n",
    "        #print(type(i))\n",
    "        if i not in dictionar:\n",
    "            dictionar[i] = str(k)\n",
    "            k = k+1\n",
    "    for i in wordssplitted2:\n",
    "        #print(type(i))\n",
    "        if i not in dictionar:\n",
    "            dictionar[i] = str(k)\n",
    "            k = k+1\n",
    "#dictionar\n",
    "import json\n",
    "\n",
    "with open('Dictionary.json', 'w') as fp:\n",
    "    json.dump(dictionar, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12741'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data)\n",
    "#dictions = pd.DataFrame(diction)\n",
    "diction['1913']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diction)\n",
    "type(diction['hom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the inverted Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionar2 = {}\n",
    "length = 0\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    #print(wordssplitted1, wordssplitted2)\n",
    "    for j in wordssplitted1:\n",
    "        code = diction[j]\n",
    "        if code not in dictionar2:\n",
    "            dictionar2[code] = [file]\n",
    "        elif file not in dictionar2[code]:\n",
    "            dictionar2[code].append(file)\n",
    "    for j in wordssplitted2:\n",
    "        code = diction[j]\n",
    "        if code not in dictionar2:\n",
    "            dictionar2[code] = [file]\n",
    "        elif file not in dictionar2[code]:\n",
    "            dictionar2[code].append(file)\n",
    "#dictionar\n",
    "import json\n",
    "\n",
    "with open('Dictionary1.json', 'w') as fp:\n",
    "    json.dump(dictionar2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)\n",
    "#dictions = pd.DataFrame(diction)\n",
    "#diction2['0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diction['2019']\n",
    "len(diction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cleantsv/filmclean-122.tsv',\n",
       " 'Cleantsv/filmclean-193.tsv',\n",
       " 'Cleantsv/filmclean-257.tsv',\n",
       " 'Cleantsv/filmclean-463.tsv',\n",
       " 'Cleantsv/filmclean-656.tsv',\n",
       " 'Cleantsv/filmclean-767.tsv',\n",
       " 'Cleantsv/filmclean-871.tsv',\n",
       " 'Cleantsv/filmclean-895.tsv',\n",
       " 'Cleantsv/filmclean-1176.tsv',\n",
       " 'Cleantsv/filmclean-1384.tsv',\n",
       " 'Cleantsv/filmclean-1447.tsv',\n",
       " 'Cleantsv/filmclean-1489.tsv',\n",
       " 'Cleantsv/filmclean-1533.tsv',\n",
       " 'Cleantsv/filmclean-1575.tsv',\n",
       " 'Cleantsv/filmclean-1678.tsv',\n",
       " 'Cleantsv/filmclean-1695.tsv',\n",
       " 'Cleantsv/filmclean-1710.tsv',\n",
       " 'Cleantsv/filmclean-1719.tsv',\n",
       " 'Cleantsv/filmclean-2171.tsv',\n",
       " 'Cleantsv/filmclean-2177.tsv',\n",
       " 'Cleantsv/filmclean-2266.tsv',\n",
       " 'Cleantsv/filmclean-2273.tsv',\n",
       " 'Cleantsv/filmclean-2413.tsv',\n",
       " 'Cleantsv/filmclean-2514.tsv',\n",
       " 'Cleantsv/filmclean-2526.tsv',\n",
       " 'Cleantsv/filmclean-2731.tsv',\n",
       " 'Cleantsv/filmclean-2873.tsv',\n",
       " 'Cleantsv/filmclean-2922.tsv',\n",
       " 'Cleantsv/filmclean-2929.tsv',\n",
       " 'Cleantsv/filmclean-2993.tsv',\n",
       " 'Cleantsv/filmclean-3502.tsv',\n",
       " 'Cleantsv/filmclean-3686.tsv',\n",
       " 'Cleantsv/filmclean-3697.tsv',\n",
       " 'Cleantsv/filmclean-3733.tsv',\n",
       " 'Cleantsv/filmclean-3799.tsv',\n",
       " 'Cleantsv/filmclean-3832.tsv',\n",
       " 'Cleantsv/filmclean-3921.tsv',\n",
       " 'Cleantsv/filmclean-4011.tsv',\n",
       " 'Cleantsv/filmclean-4021.tsv',\n",
       " 'Cleantsv/filmclean-4150.tsv',\n",
       " 'Cleantsv/filmclean-4186.tsv',\n",
       " 'Cleantsv/filmclean-4196.tsv',\n",
       " 'Cleantsv/filmclean-4199.tsv',\n",
       " 'Cleantsv/filmclean-4214.tsv',\n",
       " 'Cleantsv/filmclean-4227.tsv',\n",
       " 'Cleantsv/filmclean-4369.tsv',\n",
       " 'Cleantsv/filmclean-4462.tsv',\n",
       " 'Cleantsv/filmclean-4562.tsv',\n",
       " 'Cleantsv/filmclean-4611.tsv',\n",
       " 'Cleantsv/filmclean-4949.tsv',\n",
       " 'Cleantsv/filmclean-5258.tsv',\n",
       " 'Cleantsv/filmclean-5265.tsv',\n",
       " 'Cleantsv/filmclean-5328.tsv',\n",
       " 'Cleantsv/filmclean-5329.tsv',\n",
       " 'Cleantsv/filmclean-5433.tsv',\n",
       " 'Cleantsv/filmclean-5495.tsv',\n",
       " 'Cleantsv/filmclean-5511.tsv',\n",
       " 'Cleantsv/filmclean-5517.tsv',\n",
       " 'Cleantsv/filmclean-5591.tsv',\n",
       " 'Cleantsv/filmclean-5644.tsv',\n",
       " 'Cleantsv/filmclean-5651.tsv',\n",
       " 'Cleantsv/filmclean-5677.tsv',\n",
       " 'Cleantsv/filmclean-5698.tsv',\n",
       " 'Cleantsv/filmclean-5722.tsv',\n",
       " 'Cleantsv/filmclean-5734.tsv',\n",
       " 'Cleantsv/filmclean-5773.tsv',\n",
       " 'Cleantsv/filmclean-5871.tsv',\n",
       " 'Cleantsv/filmclean-5894.tsv',\n",
       " 'Cleantsv/filmclean-5962.tsv',\n",
       " 'Cleantsv/filmclean-5968.tsv',\n",
       " 'Cleantsv/filmclean-6068.tsv',\n",
       " 'Cleantsv/filmclean-6094.tsv',\n",
       " 'Cleantsv/filmclean-6100.tsv',\n",
       " 'Cleantsv/filmclean-6250.tsv',\n",
       " 'Cleantsv/filmclean-6317.tsv',\n",
       " 'Cleantsv/filmclean-6430.tsv',\n",
       " 'Cleantsv/filmclean-6487.tsv',\n",
       " 'Cleantsv/filmclean-6538.tsv',\n",
       " 'Cleantsv/filmclean-6629.tsv',\n",
       " 'Cleantsv/filmclean-6706.tsv',\n",
       " 'Cleantsv/filmclean-6712.tsv',\n",
       " 'Cleantsv/filmclean-6719.tsv',\n",
       " 'Cleantsv/filmclean-6777.tsv',\n",
       " 'Cleantsv/filmclean-6893.tsv',\n",
       " 'Cleantsv/filmclean-6899.tsv',\n",
       " 'Cleantsv/filmclean-6915.tsv',\n",
       " 'Cleantsv/filmclean-6942.tsv',\n",
       " 'Cleantsv/filmclean-6963.tsv',\n",
       " 'Cleantsv/filmclean-6996.tsv',\n",
       " 'Cleantsv/filmclean-6999.tsv',\n",
       " 'Cleantsv/filmclean-7016.tsv',\n",
       " 'Cleantsv/filmclean-7078.tsv',\n",
       " 'Cleantsv/filmclean-7083.tsv',\n",
       " 'Cleantsv/filmclean-7092.tsv',\n",
       " 'Cleantsv/filmclean-7156.tsv',\n",
       " 'Cleantsv/filmclean-7164.tsv',\n",
       " 'Cleantsv/filmclean-7167.tsv',\n",
       " 'Cleantsv/filmclean-7181.tsv',\n",
       " 'Cleantsv/filmclean-7188.tsv',\n",
       " 'Cleantsv/filmclean-7195.tsv',\n",
       " 'Cleantsv/filmclean-7208.tsv',\n",
       " 'Cleantsv/filmclean-7229.tsv',\n",
       " 'Cleantsv/filmclean-7235.tsv',\n",
       " 'Cleantsv/filmclean-7249.tsv',\n",
       " 'Cleantsv/filmclean-7257.tsv',\n",
       " 'Cleantsv/filmclean-7270.tsv',\n",
       " 'Cleantsv/filmclean-7273.tsv',\n",
       " 'Cleantsv/filmclean-7715.tsv',\n",
       " 'Cleantsv/filmclean-7833.tsv',\n",
       " 'Cleantsv/filmclean-8376.tsv',\n",
       " 'Cleantsv/filmclean-9083.tsv',\n",
       " 'Cleantsv/filmclean-9275.tsv',\n",
       " 'Cleantsv/filmclean-9368.tsv',\n",
       " 'Cleantsv/filmclean-9433.tsv',\n",
       " 'Cleantsv/filmclean-9816.tsv',\n",
       " 'Cleantsv/filmclean-9834.tsv',\n",
       " 'Cleantsv/filmclean-10464.tsv',\n",
       " 'Cleantsv/filmclean-10465.tsv',\n",
       " 'Cleantsv/filmclean-10466.tsv',\n",
       " 'Cleantsv/filmclean-10467.tsv',\n",
       " 'Cleantsv/filmclean-10472.tsv',\n",
       " 'Cleantsv/filmclean-10473.tsv',\n",
       " 'Cleantsv/filmclean-11045.tsv',\n",
       " 'Cleantsv/filmclean-11250.tsv',\n",
       " 'Cleantsv/filmclean-11540.tsv',\n",
       " 'Cleantsv/filmclean-11598.tsv',\n",
       " 'Cleantsv/filmclean-13027.tsv',\n",
       " 'Cleantsv/filmclean-13980.tsv',\n",
       " 'Cleantsv/filmclean-14412.tsv',\n",
       " 'Cleantsv/filmclean-15725.tsv',\n",
       " 'Cleantsv/filmclean-16298.tsv',\n",
       " 'Cleantsv/filmclean-16612.tsv',\n",
       " 'Cleantsv/filmclean-16784.tsv',\n",
       " 'Cleantsv/filmclean-17336.tsv',\n",
       " 'Cleantsv/filmclean-17715.tsv',\n",
       " 'Cleantsv/filmclean-17777.tsv',\n",
       " 'Cleantsv/filmclean-18664.tsv',\n",
       " 'Cleantsv/filmclean-19249.tsv',\n",
       " 'Cleantsv/filmclean-19455.tsv',\n",
       " 'Cleantsv/filmclean-19510.tsv',\n",
       " 'Cleantsv/filmclean-19663.tsv',\n",
       " 'Cleantsv/filmclean-19724.tsv',\n",
       " 'Cleantsv/filmclean-19811.tsv',\n",
       " 'Cleantsv/filmclean-19884.tsv',\n",
       " 'Cleantsv/filmclean-19895.tsv',\n",
       " 'Cleantsv/filmclean-20434.tsv',\n",
       " 'Cleantsv/filmclean-20871.tsv',\n",
       " 'Cleantsv/filmclean-20949.tsv',\n",
       " 'Cleantsv/filmclean-21077.tsv',\n",
       " 'Cleantsv/filmclean-21173.tsv',\n",
       " 'Cleantsv/filmclean-21271.tsv',\n",
       " 'Cleantsv/filmclean-22512.tsv',\n",
       " 'Cleantsv/filmclean-22513.tsv',\n",
       " 'Cleantsv/filmclean-22530.tsv',\n",
       " 'Cleantsv/filmclean-22602.tsv',\n",
       " 'Cleantsv/filmclean-22832.tsv',\n",
       " 'Cleantsv/filmclean-23090.tsv',\n",
       " 'Cleantsv/filmclean-23247.tsv',\n",
       " 'Cleantsv/filmclean-23362.tsv',\n",
       " 'Cleantsv/filmclean-23440.tsv',\n",
       " 'Cleantsv/filmclean-25380.tsv',\n",
       " 'Cleantsv/filmclean-25560.tsv',\n",
       " 'Cleantsv/filmclean-25777.tsv',\n",
       " 'Cleantsv/filmclean-25975.tsv',\n",
       " 'Cleantsv/filmclean-25988.tsv',\n",
       " 'Cleantsv/filmclean-26106.tsv',\n",
       " 'Cleantsv/filmclean-26134.tsv',\n",
       " 'Cleantsv/filmclean-26238.tsv',\n",
       " 'Cleantsv/filmclean-26307.tsv',\n",
       " 'Cleantsv/filmclean-26408.tsv',\n",
       " 'Cleantsv/filmclean-26418.tsv',\n",
       " 'Cleantsv/filmclean-26475.tsv',\n",
       " 'Cleantsv/filmclean-26573.tsv',\n",
       " 'Cleantsv/filmclean-26599.tsv',\n",
       " 'Cleantsv/filmclean-26787.tsv',\n",
       " 'Cleantsv/filmclean-27073.tsv',\n",
       " 'Cleantsv/filmclean-27641.tsv',\n",
       " 'Cleantsv/filmclean-27944.tsv',\n",
       " 'Cleantsv/filmclean-27961.tsv',\n",
       " 'Cleantsv/filmclean-28002.tsv',\n",
       " 'Cleantsv/filmclean-28095.tsv',\n",
       " 'Cleantsv/filmclean-28099.tsv',\n",
       " 'Cleantsv/filmclean-28115.tsv',\n",
       " 'Cleantsv/filmclean-28163.tsv',\n",
       " 'Cleantsv/filmclean-28332.tsv',\n",
       " 'Cleantsv/filmclean-28958.tsv',\n",
       " 'Cleantsv/filmclean-29054.tsv']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diction2['7507']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First engine searchengine1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ross\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Superman III</td>\n",
       "      <td>Superman III               is a Briti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Superman_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Footloose (1984 film)</td>\n",
       "      <td>Footloose               is a 1984 Ame...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Footloose_(1984_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Protocol (film)</td>\n",
       "      <td>Protocol               is a 1984 Amer...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Protocol_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Splash (film)</td>\n",
       "      <td>Splash               is a 1984 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Splash_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday the 13th: A New Beginning</td>\n",
       "      <td>Friday the 13th: A New Beginning     ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Friday_the_13th:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Turning Paige</td>\n",
       "      <td>Turning Paige               is a 2001...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Turning_Paige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>The Saddest Music in the World</td>\n",
       "      <td>The Saddest Music in the World       ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Saddest_Musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Goon (film)</td>\n",
       "      <td>Goon               is a 2011 Canadian...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Goon_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Charming (film)</td>\n",
       "      <td>Charming               is a 2018 Cana...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Charming_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Goon: Last of the Enforcers</td>\n",
       "      <td>Goon: Last of the Enforcers          ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Goon:_Last_of_th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title  \\\n",
       "0                        Superman III   \n",
       "1               Footloose (1984 film)   \n",
       "2                     Protocol (film)   \n",
       "3                       Splash (film)   \n",
       "4    Friday the 13th: A New Beginning   \n",
       "..                                ...   \n",
       "351                     Turning Paige   \n",
       "352    The Saddest Music in the World   \n",
       "353                       Goon (film)   \n",
       "354                   Charming (film)   \n",
       "355       Goon: Last of the Enforcers   \n",
       "\n",
       "                                                 Intro  \\\n",
       "0             Superman III               is a Briti...   \n",
       "1             Footloose               is a 1984 Ame...   \n",
       "2             Protocol               is a 1984 Amer...   \n",
       "3             Splash               is a 1984 Americ...   \n",
       "4             Friday the 13th: A New Beginning     ...   \n",
       "..                                                 ...   \n",
       "351           Turning Paige               is a 2001...   \n",
       "352           The Saddest Music in the World       ...   \n",
       "353           Goon               is a 2011 Canadian...   \n",
       "354           Charming               is a 2018 Cana...   \n",
       "355           Goon: Last of the Enforcers          ...   \n",
       "\n",
       "                                                   Url  \n",
       "0           https://en.wikipedia.org/wiki/Superman_III  \n",
       "1    https://en.wikipedia.org/wiki/Footloose_(1984_...  \n",
       "2        https://en.wikipedia.org/wiki/Protocol_(film)  \n",
       "3          https://en.wikipedia.org/wiki/Splash_(film)  \n",
       "4    https://en.wikipedia.org/wiki/Friday_the_13th:...  \n",
       "..                                                 ...  \n",
       "351        https://en.wikipedia.org/wiki/Turning_Paige  \n",
       "352  https://en.wikipedia.org/wiki/The_Saddest_Musi...  \n",
       "353          https://en.wikipedia.org/wiki/Goon_(film)  \n",
       "354      https://en.wikipedia.org/wiki/Charming_(film)  \n",
       "355  https://en.wikipedia.org/wiki/Goon:_Last_of_th...  \n",
       "\n",
       "[356 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HERE i DEFINE THE FUCTION TO RECEIVE THE DATASET, GIVEN AN INPUT\n",
    "y = list(input().split())\n",
    "def searchengine1(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictiionary based on the input\n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        #print(yfinal)\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "#print(starting_values)\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "        #print(codes)\n",
    "            new = []\n",
    "            for film in final_values:\n",
    "            #print(film)\n",
    "                if film in diction2[yfinal[codes]]:\n",
    "                    new.append(film)\n",
    "            final_values = new\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url'])\n",
    "    #megaDataframe\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:\n",
    "            k=0\n",
    "            for document in final_values:\n",
    "                totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                totakeurl = int(totakeurl.replace('.tsv', ''))\n",
    "                url = urls[totakeurl]\n",
    "                temporary = pd.read_csv('Tsvfiles/'+'film'+str(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                title = temporary['title'][0]\n",
    "                intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                new_row = [title, intro, url]\n",
    "                megaDataframe.loc[k]=new_row\n",
    "                k=k+1\n",
    "            return megaDataframe\n",
    "A = searchengine1(y)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Superman III</td>\n",
       "      <td>Superman III               is a Briti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Superman_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Footloose (1984 film)</td>\n",
       "      <td>Footloose               is a 1984 Ame...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Footloose_(1984_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Protocol (film)</td>\n",
       "      <td>Protocol               is a 1984 Amer...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Protocol_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Splash (film)</td>\n",
       "      <td>Splash               is a 1984 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Splash_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday the 13th: A New Beginning</td>\n",
       "      <td>Friday the 13th: A New Beginning     ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Friday_the_13th:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Turning Paige</td>\n",
       "      <td>Turning Paige               is a 2001...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Turning_Paige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>The Saddest Music in the World</td>\n",
       "      <td>The Saddest Music in the World       ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Saddest_Musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Goon (film)</td>\n",
       "      <td>Goon               is a 2011 Canadian...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Goon_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Charming (film)</td>\n",
       "      <td>Charming               is a 2018 Cana...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Charming_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Goon: Last of the Enforcers</td>\n",
       "      <td>Goon: Last of the Enforcers          ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Goon:_Last_of_th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title  \\\n",
       "0                        Superman III   \n",
       "1               Footloose (1984 film)   \n",
       "2                     Protocol (film)   \n",
       "3                       Splash (film)   \n",
       "4    Friday the 13th: A New Beginning   \n",
       "..                                ...   \n",
       "351                     Turning Paige   \n",
       "352    The Saddest Music in the World   \n",
       "353                       Goon (film)   \n",
       "354                   Charming (film)   \n",
       "355       Goon: Last of the Enforcers   \n",
       "\n",
       "                                                 Intro  \\\n",
       "0             Superman III               is a Briti...   \n",
       "1             Footloose               is a 1984 Ame...   \n",
       "2             Protocol               is a 1984 Amer...   \n",
       "3             Splash               is a 1984 Americ...   \n",
       "4             Friday the 13th: A New Beginning     ...   \n",
       "..                                                 ...   \n",
       "351           Turning Paige               is a 2001...   \n",
       "352           The Saddest Music in the World       ...   \n",
       "353           Goon               is a 2011 Canadian...   \n",
       "354           Charming               is a 2018 Cana...   \n",
       "355           Goon: Last of the Enforcers          ...   \n",
       "\n",
       "                                                   Url  \n",
       "0           https://en.wikipedia.org/wiki/Superman_III  \n",
       "1    https://en.wikipedia.org/wiki/Footloose_(1984_...  \n",
       "2        https://en.wikipedia.org/wiki/Protocol_(film)  \n",
       "3          https://en.wikipedia.org/wiki/Splash_(film)  \n",
       "4    https://en.wikipedia.org/wiki/Friday_the_13th:...  \n",
       "..                                                 ...  \n",
       "351        https://en.wikipedia.org/wiki/Turning_Paige  \n",
       "352  https://en.wikipedia.org/wiki/The_Saddest_Musi...  \n",
       "353          https://en.wikipedia.org/wiki/Goon_(film)  \n",
       "354      https://en.wikipedia.org/wiki/Charming_(film)  \n",
       "355  https://en.wikipedia.org/wiki/Goon:_Last_of_th...  \n",
       "\n",
       "[356 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yfinal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-36d220458db3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myfinal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'yfinal' is not defined"
     ]
    }
   ],
   "source": [
    "yfinal[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building of the second search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "#file1 = open(file, encoding=\"utf8\") \n",
    "  #  line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "   # words = line.split('\\t') \n",
    "   # wordssplitted1 = words[14].split()\n",
    "   # wordssplitted2 = words[15].split()\n",
    "    #L=wordssplitted1+wordssplitted2\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc)\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / (0.01 + n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\leona\\anaconda3\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\leona\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next cvell is a try for engine2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lina\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shining Through</td>\n",
       "      <td>Shining Through               is an A...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Shining_Through</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Believer (film)</td>\n",
       "      <td>The Believer               is a 2001 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Believer_(film)</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Swept Away (2002 film)</td>\n",
       "      <td>Swept Away               is a 2002   ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Swept_Away_(2002...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City of Ember</td>\n",
       "      <td>City of Ember               is a 2008...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/City_of_Ember</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dracula Reborn</td>\n",
       "      <td>Dracula Reborn               is a 201...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dracula_Reborn</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ouija: Origin of Evil</td>\n",
       "      <td>Ouija: Origin of Evil               i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ouija:_Origin_of...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Lady Vanishes</td>\n",
       "      <td>The Lady Vanishes               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Lady_Vanishes</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Suspicion (1941 film)</td>\n",
       "      <td>Suspicion               is a 1941    ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Suspicion_(1941_...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Constant Nymph (1943 film)</td>\n",
       "      <td>The Constant Nymph               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Constant_Nym...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Unseen (1945 film)</td>\n",
       "      <td>The Unseen               is a 1945 Am...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Unseen_(1945...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Spiral Staircase (1946 film)</td>\n",
       "      <td>The Spiral Staircase               is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Spiral_Stair...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Lady Takes a Sailor</td>\n",
       "      <td>The Lady Takes a Sailor              ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Lady_Takes_a...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Singin' in the Rain</td>\n",
       "      <td>Singin' in the Rain               is ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Singin%27_in_the...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Naked Spur</td>\n",
       "      <td>The Naked Spur               is a 195...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Naked_Spur</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Lady Vanishes (1979 film)</td>\n",
       "      <td>The Lady Vanishes               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Lady_Vanishe...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Summer Lovers</td>\n",
       "      <td>Summer Lovers               is a 1982...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Summer_Lovers</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Grimsby (film)</td>\n",
       "      <td>Grimsby               (released in th...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grimsby_(film)</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mambo Italiano (film)</td>\n",
       "      <td>Mambo Italiano               is a 200...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mambo_Italiano_(...</td>\n",
       "      <td>[00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title  \\\n",
       "0                    Shining Through   \n",
       "1                The Believer (film)   \n",
       "2             Swept Away (2002 film)   \n",
       "3                      City of Ember   \n",
       "4                     Dracula Reborn   \n",
       "5              Ouija: Origin of Evil   \n",
       "6                  The Lady Vanishes   \n",
       "7              Suspicion (1941 film)   \n",
       "8     The Constant Nymph (1943 film)   \n",
       "9             The Unseen (1945 film)   \n",
       "10  The Spiral Staircase (1946 film)   \n",
       "11           The Lady Takes a Sailor   \n",
       "12               Singin' in the Rain   \n",
       "13                    The Naked Spur   \n",
       "14     The Lady Vanishes (1979 film)   \n",
       "15                     Summer Lovers   \n",
       "16                    Grimsby (film)   \n",
       "17             Mambo Italiano (film)   \n",
       "\n",
       "                                                Intro  \\\n",
       "0            Shining Through               is an A...   \n",
       "1            The Believer               is a 2001 ...   \n",
       "2            Swept Away               is a 2002   ...   \n",
       "3            City of Ember               is a 2008...   \n",
       "4            Dracula Reborn               is a 201...   \n",
       "5            Ouija: Origin of Evil               i...   \n",
       "6            The Lady Vanishes               is a ...   \n",
       "7            Suspicion               is a 1941    ...   \n",
       "8            The Constant Nymph               is a...   \n",
       "9            The Unseen               is a 1945 Am...   \n",
       "10           The Spiral Staircase               is...   \n",
       "11           The Lady Takes a Sailor              ...   \n",
       "12           Singin' in the Rain               is ...   \n",
       "13           The Naked Spur               is a 195...   \n",
       "14           The Lady Vanishes               is a ...   \n",
       "15           Summer Lovers               is a 1982...   \n",
       "16           Grimsby               (released in th...   \n",
       "17           Mambo Italiano               is a 200...   \n",
       "\n",
       "                                                  Url  \\\n",
       "0       https://en.wikipedia.org/wiki/Shining_Through   \n",
       "1   https://en.wikipedia.org/wiki/The_Believer_(film)   \n",
       "2   https://en.wikipedia.org/wiki/Swept_Away_(2002...   \n",
       "3         https://en.wikipedia.org/wiki/City_of_Ember   \n",
       "4        https://en.wikipedia.org/wiki/Dracula_Reborn   \n",
       "5   https://en.wikipedia.org/wiki/Ouija:_Origin_of...   \n",
       "6     https://en.wikipedia.org/wiki/The_Lady_Vanishes   \n",
       "7   https://en.wikipedia.org/wiki/Suspicion_(1941_...   \n",
       "8   https://en.wikipedia.org/wiki/The_Constant_Nym...   \n",
       "9   https://en.wikipedia.org/wiki/The_Unseen_(1945...   \n",
       "10  https://en.wikipedia.org/wiki/The_Spiral_Stair...   \n",
       "11  https://en.wikipedia.org/wiki/The_Lady_Takes_a...   \n",
       "12  https://en.wikipedia.org/wiki/Singin%27_in_the...   \n",
       "13       https://en.wikipedia.org/wiki/The_Naked_Spur   \n",
       "14  https://en.wikipedia.org/wiki/The_Lady_Vanishe...   \n",
       "15        https://en.wikipedia.org/wiki/Summer_Lovers   \n",
       "16       https://en.wikipedia.org/wiki/Grimsby_(film)   \n",
       "17  https://en.wikipedia.org/wiki/Mambo_Italiano_(...   \n",
       "\n",
       "                                                Score  \n",
       "0   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "1   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "2   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "3   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "4   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "5   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "6   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "7   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "8   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "9   [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "10  [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "11  [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "12  [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "13  [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "14  [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "15  [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "16  [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  \n",
       "17  [00, 000, 0000, 00006cbd68f, 0008, 000dm, 000t...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "y = list(input().split())\n",
    "def searchengine2(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictionary based on the input\n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "            for film in final_values:\n",
    "                if film not in diction2[yfinal[codes]]:\n",
    "                    final_values.remove(film)\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url','Score'])\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:\n",
    "            k=0\n",
    "            for document in final_values:\n",
    "                totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                totakeurl = int(totakeurl.replace('.tsv', ''))\n",
    "                url = urls[totakeurl]\n",
    "                temporary = pd.read_csv('Tsvfiles/'+'film'+str(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                with open(document, encoding=\"utf8\") as file:\n",
    "                    line = file.read()\n",
    "                words = line.split('\\t') \n",
    "                wordssplitted11 = words[14].split()\n",
    "                wordssplitted21 = words[15].split()\n",
    "                s2=str(wordssplitted11)+str(wordssplitted21)\n",
    "                tfidf = TfidfVectorizer()\n",
    "                s1 = diction.keys()\n",
    "                response = tfidf.fit_transform([s2, str(s1)])\n",
    "                feature_names = tfidf.get_feature_names()\n",
    "               # for col in response.nonzero()[1]:\n",
    "                    #print (feature_names[col], ' - ', response[0, col])\n",
    "                title = temporary['title'][0]\n",
    "                intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                new_row = [title, intro, url, feature_names]\n",
    "                megaDataframe.loc[k]=new_row\n",
    "                k=k+1\n",
    "            return (megaDataframe)\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)\n",
    "#print(Docum_and_words) \n",
    "#I have the lis of documents with intro and plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 midnight 1983 american crime horror thriller film 3 direct j lee thompson screenplay origin written william robert film star charl bronson lead role support cast includ lisa eilbach andrew steven gene davi geoffrey lewi wilford brimley 10 midnight releas citi film subsidiari cannon film american cinema march 11 1983 warren staci gene davi young offic equip repairman kill women reject sexual advanc attempt flirt alway seen creepi women result frequent reject 4 first victim betti june gilbert offic worker acquaint track wood area observ sex boyfriend ambush coupl kill boyfriend give chase nake woman catch stab death 4 two lo angel polic detect leo kessler charl bronson paul mcann andrew steven investig murder kessler season veteran forc mcann consider younger 4 staci avoid prosecut construct sound alibi assault victim nake except pair latex glove hide fingerprint thu minim evid lauri kessler lisa eilbach daughter leo acquaint victim student nurs becom target killer 4 mcann refus go along kessler plant evid order frame suspect staci goe anoth rampag kill three nurs student friend kessler daughter eventu caught stark nake street staci boast say thing prove crazi hear voic order thing etc one day back street kessler well whole fuck world hear kessler repli shoot staci forehead execut leav consider asid kessler stand bodi surround polic'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Docum_and_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "#file1 = open(file, encoding=\"utf8\") \n",
    "  #  line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "   # words = line.split('\\t') \n",
    "   # wordssplitted1 = words[14].split()\n",
    "   # wordssplitted2 = words[15].split()\n",
    "    #L=wordssplitted1+wordssplitted2\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc)\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / float(n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary with idf to save computational costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "dicontain = {}\n",
    "for i in diction:\n",
    "    #if i not in dicontain:\n",
    "        dicontain[i] = math.log(30000/n_containing(i, Docum_and_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicontain.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['right', 'move', '1983', 'american', 'sport', 'drama', 'film', 'direct', 'michael', 'chapman', 'star', 'tom', 'cruis', 'craig', 'nelson', 'lea', 'thompson', 'chri', 'penn', 'gari', 'graham', 'film', 'locat', 'johnstown', 'pennsylvania', '3', '4', '5', 'pittsburgh', 'stefen', 'stef', 'djordjev', 'cruis', 'serbian', 'american', 'high', 'school', 'defens', 'back', 'gift', 'sport', 'academ', 'seek', 'colleg', 'footbal', 'scholarship', 'escap', 'econom', 'depress', 'small', 'western', 'pennsylvania', 'town', 'ampip', 'dead', 'end', 'job', 'life', 'work', 'mill', 'like', 'father', 'brother', 'greg', 'dream', 'becom', 'engin', 'right', 'graduat', 'colleg', 'ampip', 'compani', 'town', 'whose', 'economi', 'domin', 'town', 'main', 'employ', 'american', 'pipe', 'steel', 'steel', 'mill', 'struggl', 'downturn', 'earli', '1980', 'recess', 'stef', 'get', 'day', 'love', 'girlfriend', 'lisa', 'lietzk', 'thompson', 'strong', 'bond', 'teammat', 'film', 'take', 'place', 'big', 'footbal', 'game', 'undef', 'walnut', 'height', 'high', 'school', 'ampip', 'appear', 'head', 'win', 'game', 'fumbl', 'handoff', 'close', 'second', 'well', 'stefen', 'pass', 'interfer', 'penalti', 'earlier', 'game', 'lead', 'walnut', 'height', 'victori', 'follow', 'game', 'coach', 'burt', 'nickerson', 'nelson', 'lambast', 'fumbler', 'locker', 'room', 'tell', 'quit', 'game', 'stefen', 'retort', 'coach', 'quit', 'coach', 'kick', 'team', 'aftermath', 'disgruntl', 'ampip', 'fan', 'vandal', 'coach', 'nickerson', 'hous', 'yard', 'stefen', 'present', 'reluct', 'particip', 'nonetheless', 'seen', 'nickerson', 'vandal', 'flee', 'stefen', 'deal', 'person', 'battl', 'includ', 'deal', 'coach', 'blackbal', 'among', 'colleg', 'attitud', 'particip', 'desecr', 'nickerson', 'yard', 'hous', 'stefen', 'get', 'argument', 'lisa', 'best', 'friend', 'brian', 'penn', 'declin', 'scholarship', 'offer', 'usc', 'plan', 'marri', 'pregnant', 'girlfriend', 'frustrat', 'nickerson', 'stefan', 'angrili', 'confront', 'former', 'coach', 'end', 'shout', 'match', 'street', 'lisa', 'decid', 'talk', 'nickerson', 'wife', 'tri', 'help', 'end', 'nickerson', 'realiz', 'wrong', 'blackbal', 'stefan', 'accept', 'coach', 'posit', 'west', 'coast', 'cal', 'poli', 'san', 'lui', 'obispo', 'offer', 'stefen', 'full', 'scholarship', 'play', 'footbal', 'accept']\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Docum_and_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015092905232944184"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('kill',Docum_and_words[0], Docum_and_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "midnight\n",
      "1983\n",
      "american\n",
      "crime\n",
      "horror\n",
      "thriller\n",
      "film\n",
      "3\n",
      "direct\n",
      "j\n",
      "lee\n",
      "thompson\n",
      "screenplay\n",
      "origin\n",
      "written\n",
      "william\n",
      "robert\n",
      "film\n",
      "star\n",
      "charl\n",
      "bronson\n",
      "lead\n",
      "role\n",
      "support\n",
      "cast\n",
      "includ\n",
      "lisa\n",
      "eilbach\n",
      "andrew\n",
      "steven\n",
      "gene\n",
      "davi\n",
      "geoffrey\n",
      "lewi\n",
      "wilford\n",
      "brimley\n",
      "10\n",
      "midnight\n",
      "releas\n",
      "citi\n",
      "film\n",
      "subsidiari\n",
      "cannon\n",
      "film\n",
      "american\n",
      "cinema\n",
      "march\n",
      "11\n",
      "1983\n",
      "warren\n",
      "staci\n",
      "gene\n",
      "davi\n",
      "young\n",
      "offic\n",
      "equip\n",
      "repairman\n",
      "kill\n",
      "women\n",
      "reject\n",
      "sexual\n",
      "advanc\n",
      "attempt\n",
      "flirt\n",
      "alway\n",
      "seen\n",
      "creepi\n",
      "women\n",
      "result\n",
      "frequent\n",
      "reject\n",
      "4\n",
      "first\n",
      "victim\n",
      "betti\n",
      "june\n",
      "gilbert\n",
      "offic\n",
      "worker\n",
      "acquaint\n",
      "track\n",
      "wood\n",
      "area\n",
      "observ\n",
      "sex\n",
      "boyfriend\n",
      "ambush\n",
      "coupl\n",
      "kill\n",
      "boyfriend\n",
      "give\n",
      "chase\n",
      "nake\n",
      "woman\n",
      "catch\n",
      "stab\n",
      "death\n",
      "4\n",
      "two\n",
      "lo\n",
      "angel\n",
      "polic\n",
      "detect\n",
      "leo\n",
      "kessler\n",
      "charl\n",
      "bronson\n",
      "paul\n",
      "mcann\n",
      "andrew\n",
      "steven\n",
      "investig\n",
      "murder\n",
      "kessler\n",
      "season\n",
      "veteran\n",
      "forc\n",
      "mcann\n",
      "consider\n",
      "younger\n",
      "4\n",
      "staci\n",
      "avoid\n",
      "prosecut\n",
      "construct\n",
      "sound\n",
      "alibi\n",
      "assault\n",
      "victim\n",
      "nake\n",
      "except\n",
      "pair\n",
      "latex\n",
      "glove\n",
      "hide\n",
      "fingerprint\n",
      "thu\n",
      "minim\n",
      "evid\n",
      "lauri\n",
      "kessler\n",
      "lisa\n",
      "eilbach\n",
      "daughter\n",
      "leo\n",
      "acquaint\n",
      "victim\n",
      "student\n",
      "nurs\n",
      "becom\n",
      "target\n",
      "killer\n",
      "4\n",
      "mcann\n",
      "refus\n",
      "go\n",
      "along\n",
      "kessler\n",
      "plant\n",
      "evid\n",
      "order\n",
      "frame\n",
      "suspect\n",
      "staci\n",
      "goe\n",
      "anoth\n",
      "rampag\n",
      "kill\n",
      "three\n",
      "nurs\n",
      "student\n",
      "friend\n",
      "kessler\n",
      "daughter\n",
      "eventu\n",
      "caught\n",
      "stark\n",
      "nake\n",
      "street\n",
      "staci\n",
      "boast\n",
      "say\n",
      "thing\n",
      "prove\n",
      "crazi\n",
      "hear\n",
      "voic\n",
      "order\n",
      "thing\n",
      "etc\n",
      "one\n",
      "day\n",
      "back\n",
      "street\n",
      "kessler\n",
      "well\n",
      "whole\n",
      "fuck\n",
      "world\n",
      "hear\n",
      "kessler\n",
      "repli\n",
      "shoot\n",
      "staci\n",
      "forehead\n",
      "execut\n",
      "leav\n",
      "consider\n",
      "asid\n",
      "kessler\n",
      "stand\n",
      "bodi\n",
      "surround\n",
      "polic\n"
     ]
    }
   ],
   "source": [
    "for i in Docum_and_words[0].split():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionar3 = {}\n",
    "#length = 0\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    for j in Docum_and_words[i].split():\n",
    "            code = diction[j]\n",
    "            if code not in dictionar3:\n",
    "                dictionar3[code] = [file, tf(j, Docum_and_words[i])*dicontain[j]]\n",
    "            elif file not in dictionar3[code]:\n",
    "                dictionar3[code].append([file, tf(j, Docum_and_words[i])*dicontain[j]])\n",
    "import json\n",
    "\n",
    "with open('Dictionary2.json', 'w') as fp:\n",
    "    json.dump(dictionar3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionar3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
