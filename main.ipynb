{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.1] Get the list of movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start downloading the html files with inside the urls of the documents we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/10_to_Midnight'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies2.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "urls = []\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies1.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "lst_a\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[10000]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Z.P.G.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open('C:/Users/leona/Desktop/ADMHMK-3/movies3.html'), \"html.parser\")\n",
    "soup.head()\n",
    "lst_a = soup.select('a')\n",
    "lst_a\n",
    "for i in lst_a:\n",
    "    urls.append(i.get('href'))\n",
    "urls[20000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Whistle_(2003_film)'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[29999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all 30.000 urls we need to save them in dictionary that we will access later when we want to print the urls in the output of the search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicturls = {}\n",
    "for i in range(len(urls)):\n",
    "    dicturls[i] = urls[i]\n",
    "with open('dicturls.json', 'w') as fp:\n",
    "    json.dump(dicturls, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Whistle_(2003_film)'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicturls[str(29999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.2] Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, avoiding any error,(there will not be, and knowing this I already saved in order the urls), we download on our pc the html files we need for our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "import time\n",
    "def getwikipageshtml(urls):\n",
    "    k=0\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            ur_l = requests.get(urls[i])\n",
    "            soup = BeautifulSoup(ur_l.content, 'html.parser')\n",
    "            soup = soup.prettify(\"utf-8\")   \n",
    "            stringa = 'Articles/article-'+str(k)+'.html'\n",
    "            k = k+1\n",
    "            Html_file= open(stringa, \"wb\")\n",
    "            Html_file.write(soup)\n",
    "            Html_file.close()\n",
    "        except(URLError,HTTPError, ContentTooShortError)  as e:\n",
    "            html = None\n",
    "        #time.sleep(0.001) #Actually for this task we don't need to stop anytime\n",
    "    return\n",
    "getwikipageshtml(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.3] Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we access every html document and take out the informations we need to build our tsv files with info title, intro, plot, film_name, director, producer, writer, starring, music, release date, runtime, country, language, budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "for i in range(30000):\n",
    "    filename = \"Articles/article-\"+str(i)+\".html\"\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        #Now That I opened the file I have to look for the section asked\n",
    "        #first I take the title and clear him of spaces and the word -Wikipedia\n",
    "        titlepage = soup.title.string\n",
    "        titleonly = titlepage.split(\"- Wikipedia\")\n",
    "        titlepage = titleonly[0].strip()\n",
    "        #now I create empty string as intro and plot\n",
    "        intro = ''\n",
    "        plot = ''\n",
    "        #Now i searcvh the first paragraph that usually or is empty or is the intro\n",
    "        start = soup.find('p')\n",
    "        intro = start\n",
    "        intro1 = start.text\n",
    "        B = ''\n",
    "        #in B I put m,y limit for the paragraphs in the intro, because after this h2 there will always be the plot\n",
    "        B = intro.find_next_sibling('h2')\n",
    "        if(B!=None and B.find_next_sibling('p')):\n",
    "            C = B.find_next_sibling('p')\n",
    "            while(C != intro.find_next_sibling('p')): \n",
    "                intro1 = intro1 + intro.find_next_sibling('p').text\n",
    "                intro = intro.find_next_sibling('p')\n",
    "            plot = ''    \n",
    "            #then i do the same with the plot, so I start at B and end in the next h2\n",
    "            plot = B\n",
    "            plot1 = ''\n",
    "            compare = ''\n",
    "            if(B.find_next_sibling('h2')):\n",
    "                compare = B.find_next_sibling('h2')\n",
    "                compareto = compare.find_next_sibling('p')\n",
    "                while(compareto != plot.find_next_sibling('p')):\n",
    "                    plot1 = plot1 + plot.find_next_sibling('p').text\n",
    "                    #print(plot1)\n",
    "                    plot = plot.find_next_sibling('p')\n",
    "                    #if plot or intro are empty I put NA\n",
    "        if(intro1 == ''):\n",
    "            intro1 = \"NA\"\n",
    "        if plot1 == '':\n",
    "            plot1 = \"NA\"\n",
    "        #Now I start working on the other features\n",
    "        intro = intro1\n",
    "        plot = plot1\n",
    "        film_name = 'NA'\n",
    "        director = \"NA\"\n",
    "        producer = \"NA\"\n",
    "        writer = \"NA\"\n",
    "        starring = \"NA\"\n",
    "        music = \"NA\"\n",
    "        release_date = \"NA\"\n",
    "        runtime = \"NA\"\n",
    "        country = \"NA\"\n",
    "        language = \"NA\"\n",
    "        budget = \"NA\"\n",
    "#'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'\n",
    "        for link in soup.find_all('tr'):\n",
    "            if soup.find('th',{'class': ['summary']})!= None:\n",
    "                    film_name = soup.find('th',{'class': ['summary']} ).text.strip()\n",
    "            if link.th:\n",
    "#I just check in the th and if I find the class I need I take the relative td. Some of them are inaccessible so the if link.td\n",
    "                if(link.th.text.strip() == 'Directed by'):\n",
    "                     director = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Produced by'):\n",
    "                      producer = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Written by'):\n",
    "                    writer = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Starring'):\n",
    "                    starring = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Music by'):\n",
    "                     music = link.td.text.strip()               \n",
    "                elif(link.th.text.strip() == 'Release date'):\n",
    "                    if link.td:\n",
    "                        release_date = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Running time'):\n",
    "                    runtime = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Country'):\n",
    "                    if link.td:\n",
    "                        country = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Language'):\n",
    "                    if link.td:\n",
    "                        language = link.td.text.strip()\n",
    "                elif(link.th.text.strip() == 'Budget'):\n",
    "                    budget = link.td.text.strip()\n",
    "#now I open the tsv files and create one for every film.        \n",
    "        tsvname = 'Tsvfiles/'+'film'+str(i)+'.tsv'\n",
    "        with io.open(tsvname, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['title','intro', 'plot', 'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'])\n",
    "            tsv_writer.writerow([titlepage, intro, plot, film_name, director, producer, writer, starring, music, release_date, runtime, \n",
    "                 country, language, budget])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whistle (2003 film)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n         What Became of Jack and Jill?\\n        \\n\\n       is a 1972 British\\n       \\n        horror film\\n       \\n       directed by\\n       \\n        Bill Bain\\n       \\n       and starring\\n       \\n        Mona Washbourne\\n       \\n       ,\\n       \\n        Paul Nicholas\\n       \\n       , and\\n       \\n        Vanessa Howard\\n       \\n       .\\n       \\n\\n         [1]\\n        \\n\\n       It was part of an abandoned attempt by\\n       \\n        Amicus Pictures\\n       \\n       to compete with\\n       \\n        Hammer Studios\\n       \\n       by breaking into the\\n       \\n        grindhouse\\n       \\n       market. Studio executives were ultimately too disturbed by the final product to release it under the Amicus name, and they sold the film to\\n       \\n        20th Century Fox\\n       \\n       .\\n      '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is just to try stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean The files, so we first define a preprocess function to do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function on every section of the tsv files, except for the tiles of the categories, so from the part 13 and clean them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 midnight\n",
      "10 midnight 1983 american crime horror thriller film 3 direct j lee thompson screenplay origin written william robert film star charl bronson lead role support cast includ lisa eilbach andrew steven gene davi geoffrey lewi wilford brimley 10 midnight releas citi film subsidiari cannon film american cinema march 11 1983\n",
      "warren staci gene davi young offic equip repairman kill women reject sexual advanc attempt flirt alway seen creepi women result frequent reject 4 first victim betti june gilbert offic worker acquaint track wood area observ sex boyfriend ambush coupl kill boyfriend give chase nake woman catch stab death 4 two lo angel polic detect leo kessler charl bronson paul mcann andrew steven investig murder kessler season veteran forc mcann consider younger 4 staci avoid prosecut construct sound alibi assault victim nake except pair latex glove hide fingerprint thu minim evid lauri kessler lisa eilbach daughter leo acquaint victim student nurs becom target killer 4 mcann refus go along kessler plant evid order frame suspect staci goe anoth rampag kill three nurs student friend kessler daughter eventu caught stark nake street staci boast say thing prove crazi hear voic order thing etc one day back street kessler well whole fuck world hear kessler repli shoot staci forehead execut leav consider asid kessler stand bodi surround polic\n",
      "10 midnight\n",
      "j lee thompson\n",
      "pancho kohner lanc hool\n",
      "william robert j lee thompson\n",
      "charl bronson lisa eilbach andrew steven gene davi geoffrey lewi wilford brimley\n",
      "robert ragland\n",
      "march 11 1983 1983 03 11 u\n",
      "101 min\n",
      "unit state\n",
      "english\n",
      "4 520 000 us\n"
     ]
    }
   ],
   "source": [
    "for i in range(30000):\n",
    "    file1 = open('Tsvfiles/film'+str(i)+'.tsv', encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    words = line.split('\\t') \n",
    "    for j in range(13, len(words)):\n",
    "        words[j] = preprocess(words[j])\n",
    "        if j ==13:\n",
    "            #Here for the format of tsv files and my split('\\t') the word budget would always be in my title, so I take her out\n",
    "            words[j] = words[j].replace('budget ','')  \n",
    "            words[j] = words[j].replace('budget\\n\\n','')\n",
    "        print(words[j])\n",
    "    tsvname = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    with io.open(tsvname, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['title','intro', 'plot', 'film_name', 'director', 'producer','writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget'])\n",
    "            tsv_writer.writerow(words[13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 midnight'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from the clean intro and plots we start building the dictionary that will have \"index\":word, so we have a unique associatioin between a word in the dataset we care about and a number. Obviously we will save it as json file, as many other dictionaries that follow to be able to use them when we want in our search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionar = {}\n",
    "k = 0\n",
    "for i in range(30000):\n",
    "    file1 = open(\"Cleantsv/filmclean-\"+str(i)+'.tsv', encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    #print(wordssplitted1, wordssplitted2)\n",
    "    for i in wordssplitted1:\n",
    "        #print(type(i))\n",
    "        if i not in dictionar:\n",
    "            dictionar[i] = str(k)\n",
    "            k = k+1\n",
    "    for i in wordssplitted2:\n",
    "        #print(type(i))\n",
    "        if i not in dictionar:\n",
    "            dictionar[i] = str(k)\n",
    "            k = k+1\n",
    "#dictionar\n",
    "import json\n",
    "\n",
    "with open('Dictionary.json', 'w') as fp:\n",
    "    json.dump(dictionar, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12741'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data)\n",
    "diction['1913']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diction)\n",
    "type(diction['hom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the inverted Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the inverted dictionary that for each index has as value the nanme of the documents we know have the word that in the previous dictionary has that unique index. We save it as json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionar2 = {}\n",
    "length = 0\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    #print(wordssplitted1, wordssplitted2)\n",
    "    for j in wordssplitted1:\n",
    "        code = diction[j]\n",
    "        if code not in dictionar2:\n",
    "            dictionar2[code] = [file]\n",
    "        elif file not in dictionar2[code]:\n",
    "            dictionar2[code].append(file)\n",
    "    for j in wordssplitted2:\n",
    "        code = diction[j]\n",
    "        if code not in dictionar2:\n",
    "            dictionar2[code] = [file]\n",
    "        elif file not in dictionar2[code]:\n",
    "            dictionar2[code].append(file)\n",
    "#dictionar\n",
    "import json\n",
    "\n",
    "with open('Dictionary1.json', 'w') as fp:\n",
    "    json.dump(dictionar2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)\n",
    "#dictions = pd.DataFrame(diction)\n",
    "#diction2['0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diction['2019']\n",
    "len(diction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First engine searchengine1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we remind some funcion we need to use and the vocabularies, so the following cell must not be run, but just used to remind the instruments we are using and defining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "import pandas as pd\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)\n",
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for the engine as a function that takes a input to analyze. It will give all the documents that will match all the words of the input inside their intro or plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You should must give me the string, errrrrrrror\n"
     ]
    }
   ],
   "source": [
    "#HERE i DEFINE THE FUCTION TO RECEIVE THE DATASET, GIVEN AN INPUT\n",
    "y = list(input().split())\n",
    "def searchengine1(y):\n",
    "    if not y:\n",
    "        return print(\"You should must give me the string, errrrrrrror\")\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictiionary based on the input\n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "            new = []\n",
    "            #print(final_values)\n",
    "            for film in final_values:                \n",
    "                if film in diction2[yfinal[codes]]:\n",
    "                    new.append(film)\n",
    "            final_values = new\n",
    "            #print(final_values)\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url'])\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:           \n",
    "            k=0\n",
    "            for document in final_values:\n",
    "                totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                totakeurl = str(int(totakeurl.replace('.tsv', '')))\n",
    "                url = dicturls[totakeurl]\n",
    "                temporary = pd.read_csv('Tsvfiles/'+'film'+(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                title = temporary['title'][0]\n",
    "                intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                new_row = [title, intro, url]\n",
    "                megaDataframe.loc[k]=new_row\n",
    "                k=k+1\n",
    "            return megaDataframe\n",
    "A = searchengine1(y)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daniel craig\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turner &amp; Hooch</td>\n",
       "      <td>Turner &amp; Hooch               is a 198...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Turner_%26_Hooch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Power of One (film)</td>\n",
       "      <td>The Power of One               is a 1...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Power_of_One...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101 Dalmatians (1996 film)</td>\n",
       "      <td>101 Dalmatians               is a 199...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/101_Dalmatians_(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twisted Desire</td>\n",
       "      <td>Twisted Desire               is a 199...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Twisted_Desire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Thirteenth Floor</td>\n",
       "      <td>The Thirteenth Floor               is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Thirteenth_F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lara Croft: Tomb Raider</td>\n",
       "      <td>Lara Croft: Tomb Raider              ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lara_Croft:_Tomb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Road to Perdition</td>\n",
       "      <td>Road to Perdition               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Road_to_Perdition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Casino Royale (2006 film)</td>\n",
       "      <td>Casino Royale               is a 2006...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Casino_Royale_(2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Infamous (film)</td>\n",
       "      <td>Infamous               is a 2006 Amer...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Infamous_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Golden Compass (film)</td>\n",
       "      <td>The Golden Compass               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Golden_Compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Invasion (film)</td>\n",
       "      <td>The Invasion               is a 2007 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Invasion_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Defiance (2008 film)</td>\n",
       "      <td>Defiance               is a 2008 Amer...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Defiance_(2008_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Quantum of Solace</td>\n",
       "      <td>Quantum of Solace               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum_of_Solace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Adventures of Tintin (film)</td>\n",
       "      <td>The Adventures of Tintin             ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Adventures_o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cowboys &amp; Aliens</td>\n",
       "      <td>Cowboys &amp; Aliens               is a 2...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cowboys_%26_Aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Dream House (2011 film)</td>\n",
       "      <td>Dream House               is a 2011 A...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dream_House_(201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Girl with the Dragon Tattoo (2011 film)</td>\n",
       "      <td>The Girl with the Dragon Tattoo      ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Girl_with_th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Compliance (film)</td>\n",
       "      <td>Compliance               is a 2012 Am...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Compliance_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Identity Thief</td>\n",
       "      <td>Identity Thief               is a 201...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Identity_Thief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Snitch (film)</td>\n",
       "      <td>Snitch               is a 2013 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Snitch_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The Longest Ride (film)</td>\n",
       "      <td>The Longest Ride               is a 2...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Longest_Ride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Finest Hours (2016 film)</td>\n",
       "      <td>The Finest Hours               is a 2...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Finest_Hours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Logan Lucky</td>\n",
       "      <td>Logan Lucky               is a 2017 A...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Logan_Lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>No Time to Die</td>\n",
       "      <td>No Time to Die               is an up...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/No_Time_to_Die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I Take This Oath</td>\n",
       "      <td>I Take This Oath               is a 1...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I_Take_This_Oath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The Devil and Daniel Webster (film)</td>\n",
       "      <td>The Devil and Daniel Webster         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Devil_and_Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Dangerous Passage</td>\n",
       "      <td>Dangerous Passage               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dangerous_Passage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Beyond Glory</td>\n",
       "      <td>Beyond Glory               is a 1948 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Beyond_Glory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hit the Deck (1955 film)</td>\n",
       "      <td>Hit the Deck               is a 1955 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hit_the_Deck_(19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Dunwich Horror (film)</td>\n",
       "      <td>The Dunwich Horror               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Dunwich_Horr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Some Voices (film)</td>\n",
       "      <td>Some Voices               is a 2000 B...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Some_Voices_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The Mother (film)</td>\n",
       "      <td>The Mother               is a 2003 Br...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Mother_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Sylvia (2003 film)</td>\n",
       "      <td>Sylvia               is a 2003 Britis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sylvia_(2003_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Enduring Love (film)</td>\n",
       "      <td>Enduring Love               is a 2004...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Enduring_Love_(f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Layer Cake (film)</td>\n",
       "      <td>Layer Cake               (also occasi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Layer_Cake_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Death at a Funeral (2007 film)</td>\n",
       "      <td>Death at a Funeral               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Death_at_a_Funer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Flashbacks of a Fool</td>\n",
       "      <td>Flashbacks of a Fool               is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Flashbacks_of_a_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Skyfall</td>\n",
       "      <td>Skyfall               is a 2012 Briti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Skyfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Spectre (2015 film)</td>\n",
       "      <td>Spectre               is a 2015 Briti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Spectre_(2015_film)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Title  \\\n",
       "0                                Turner & Hooch   \n",
       "1                       The Power of One (film)   \n",
       "2                    101 Dalmatians (1996 film)   \n",
       "3                                Twisted Desire   \n",
       "4                          The Thirteenth Floor   \n",
       "5                       Lara Croft: Tomb Raider   \n",
       "6                             Road to Perdition   \n",
       "7                     Casino Royale (2006 film)   \n",
       "8                               Infamous (film)   \n",
       "9                     The Golden Compass (film)   \n",
       "10                          The Invasion (film)   \n",
       "11                         Defiance (2008 film)   \n",
       "12                            Quantum of Solace   \n",
       "13              The Adventures of Tintin (film)   \n",
       "14                             Cowboys & Aliens   \n",
       "15                      Dream House (2011 film)   \n",
       "16  The Girl with the Dragon Tattoo (2011 film)   \n",
       "17                            Compliance (film)   \n",
       "18                               Identity Thief   \n",
       "19                                Snitch (film)   \n",
       "20                      The Longest Ride (film)   \n",
       "21                 The Finest Hours (2016 film)   \n",
       "22                                  Logan Lucky   \n",
       "23                               No Time to Die   \n",
       "24                             I Take This Oath   \n",
       "25          The Devil and Daniel Webster (film)   \n",
       "26                            Dangerous Passage   \n",
       "27                                 Beyond Glory   \n",
       "28                     Hit the Deck (1955 film)   \n",
       "29                    The Dunwich Horror (film)   \n",
       "30                           Some Voices (film)   \n",
       "31                            The Mother (film)   \n",
       "32                           Sylvia (2003 film)   \n",
       "33                         Enduring Love (film)   \n",
       "34                            Layer Cake (film)   \n",
       "35               Death at a Funeral (2007 film)   \n",
       "36                         Flashbacks of a Fool   \n",
       "37                                      Skyfall   \n",
       "38                          Spectre (2015 film)   \n",
       "\n",
       "                                                Intro  \\\n",
       "0            Turner & Hooch               is a 198...   \n",
       "1            The Power of One               is a 1...   \n",
       "2            101 Dalmatians               is a 199...   \n",
       "3            Twisted Desire               is a 199...   \n",
       "4            The Thirteenth Floor               is...   \n",
       "5            Lara Croft: Tomb Raider              ...   \n",
       "6            Road to Perdition               is a ...   \n",
       "7            Casino Royale               is a 2006...   \n",
       "8            Infamous               is a 2006 Amer...   \n",
       "9            The Golden Compass               is a...   \n",
       "10           The Invasion               is a 2007 ...   \n",
       "11           Defiance               is a 2008 Amer...   \n",
       "12           Quantum of Solace               is a ...   \n",
       "13           The Adventures of Tintin             ...   \n",
       "14           Cowboys & Aliens               is a 2...   \n",
       "15           Dream House               is a 2011 A...   \n",
       "16           The Girl with the Dragon Tattoo      ...   \n",
       "17           Compliance               is a 2012 Am...   \n",
       "18           Identity Thief               is a 201...   \n",
       "19           Snitch               is a 2013 Americ...   \n",
       "20           The Longest Ride               is a 2...   \n",
       "21           The Finest Hours               is a 2...   \n",
       "22           Logan Lucky               is a 2017 A...   \n",
       "23           No Time to Die               is an up...   \n",
       "24           I Take This Oath               is a 1...   \n",
       "25           The Devil and Daniel Webster         ...   \n",
       "26           Dangerous Passage               is a ...   \n",
       "27           Beyond Glory               is a 1948 ...   \n",
       "28           Hit the Deck               is a 1955 ...   \n",
       "29           The Dunwich Horror               is a...   \n",
       "30           Some Voices               is a 2000 B...   \n",
       "31           The Mother               is a 2003 Br...   \n",
       "32           Sylvia               is a 2003 Britis...   \n",
       "33           Enduring Love               is a 2004...   \n",
       "34           Layer Cake               (also occasi...   \n",
       "35           Death at a Funeral               is a...   \n",
       "36           Flashbacks of a Fool               is...   \n",
       "37           Skyfall               is a 2012 Briti...   \n",
       "38           Spectre               is a 2015 Briti...   \n",
       "\n",
       "                                                  Url  \n",
       "0      https://en.wikipedia.org/wiki/Turner_%26_Hooch  \n",
       "1   https://en.wikipedia.org/wiki/The_Power_of_One...  \n",
       "2   https://en.wikipedia.org/wiki/101_Dalmatians_(...  \n",
       "3        https://en.wikipedia.org/wiki/Twisted_Desire  \n",
       "4   https://en.wikipedia.org/wiki/The_Thirteenth_F...  \n",
       "5   https://en.wikipedia.org/wiki/Lara_Croft:_Tomb...  \n",
       "6     https://en.wikipedia.org/wiki/Road_to_Perdition  \n",
       "7   https://en.wikipedia.org/wiki/Casino_Royale_(2...  \n",
       "8       https://en.wikipedia.org/wiki/Infamous_(film)  \n",
       "9   https://en.wikipedia.org/wiki/The_Golden_Compa...  \n",
       "10  https://en.wikipedia.org/wiki/The_Invasion_(film)  \n",
       "11  https://en.wikipedia.org/wiki/Defiance_(2008_f...  \n",
       "12    https://en.wikipedia.org/wiki/Quantum_of_Solace  \n",
       "13  https://en.wikipedia.org/wiki/The_Adventures_o...  \n",
       "14   https://en.wikipedia.org/wiki/Cowboys_%26_Aliens  \n",
       "15  https://en.wikipedia.org/wiki/Dream_House_(201...  \n",
       "16  https://en.wikipedia.org/wiki/The_Girl_with_th...  \n",
       "17    https://en.wikipedia.org/wiki/Compliance_(film)  \n",
       "18       https://en.wikipedia.org/wiki/Identity_Thief  \n",
       "19        https://en.wikipedia.org/wiki/Snitch_(film)  \n",
       "20  https://en.wikipedia.org/wiki/The_Longest_Ride...  \n",
       "21  https://en.wikipedia.org/wiki/The_Finest_Hours...  \n",
       "22          https://en.wikipedia.org/wiki/Logan_Lucky  \n",
       "23       https://en.wikipedia.org/wiki/No_Time_to_Die  \n",
       "24     https://en.wikipedia.org/wiki/I_Take_This_Oath  \n",
       "25  https://en.wikipedia.org/wiki/The_Devil_and_Da...  \n",
       "26    https://en.wikipedia.org/wiki/Dangerous_Passage  \n",
       "27         https://en.wikipedia.org/wiki/Beyond_Glory  \n",
       "28  https://en.wikipedia.org/wiki/Hit_the_Deck_(19...  \n",
       "29  https://en.wikipedia.org/wiki/The_Dunwich_Horr...  \n",
       "30   https://en.wikipedia.org/wiki/Some_Voices_(film)  \n",
       "31    https://en.wikipedia.org/wiki/The_Mother_(film)  \n",
       "32   https://en.wikipedia.org/wiki/Sylvia_(2003_film)  \n",
       "33  https://en.wikipedia.org/wiki/Enduring_Love_(f...  \n",
       "34    https://en.wikipedia.org/wiki/Layer_Cake_(film)  \n",
       "35  https://en.wikipedia.org/wiki/Death_at_a_Funer...  \n",
       "36  https://en.wikipedia.org/wiki/Flashbacks_of_a_...  \n",
       "37              https://en.wikipedia.org/wiki/Skyfall  \n",
       "38  https://en.wikipedia.org/wiki/Spectre_(2015_film)  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine1(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gothic film novel\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'searchengine1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-483cada9b73b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msearchengine1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'searchengine1' is not defined"
     ]
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine1(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building of the second search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a list with inside strings that represent intro and plot of every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()# Use this to read file content as a stream: \n",
    "    #print(line)\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)\n",
    "#print(Docum_and_words) \n",
    "#I have the lis of documents with intro and plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two function to get the tf and the number of documents containing a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary with idf to save computational costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary declarations to run the code at any moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 midnight 1983 american crime horror thriller film 3 direct j lee thompson screenplay origin written william robert film star charl bronson lead role support cast includ lisa eilbach andrew steven gene davi geoffrey lewi wilford brimley 10 midnight releas citi film subsidiari cannon film american cinema march 11 1983 warren staci gene davi young offic equip repairman kill women reject sexual advanc attempt flirt alway seen creepi women result frequent reject 4 first victim betti june gilbert offic worker acquaint track wood area observ sex boyfriend ambush coupl kill boyfriend give chase nake woman catch stab death 4 two lo angel polic detect leo kessler charl bronson paul mcann andrew steven investig murder kessler season veteran forc mcann consider younger 4 staci avoid prosecut construct sound alibi assault victim nake except pair latex glove hide fingerprint thu minim evid lauri kessler lisa eilbach daughter leo acquaint victim student nurs becom target killer 4 mcann refus go along kessler plant evid order frame suspect staci goe anoth rampag kill three nurs student friend kessler daughter eventu caught stark nake street staci boast say thing prove crazi hear voic order thing etc one day back street kessler well whole fuck world hear kessler repli shoot staci forehead execut leav consider asid kessler stand bodi surround polic'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Docum_and_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data) #first dict with every word and a unique value\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)#first inverted dict with number and list of document with a word that has that number in diction\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc.split())\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / float(n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dictionary with inside associated to every word in diction(the one that matched indexes and words) the number of documents it is in on the 30.000 of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncontain = {}\n",
    "for i in diction:\n",
    "    refer = diction[i]\n",
    "    #print(i)\n",
    "    ncontain[i] = len(diction2[refer])\n",
    "with open('ncontain.json', 'w') as fp:\n",
    "    json.dump(ncontain, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\ncontain.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "ncontain = json.loads(data) #first dict with every word and a unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ncontain.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idfdict is the dictionary that if you select a word will give you its idf. We save it as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "idfdict = {}\n",
    "for i in ncontain:\n",
    "    idfdict[i]=math.log(30000 / ncontain[i])\n",
    "with open('idfdict.json', 'w') as fp:\n",
    "    json.dump(idfdict, fp)\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\idfdict.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "idfdict = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\idfdict.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "idfdict = json.loads(data)\n",
    "#idfdict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second dictionary with the tf-idf values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to do very easily the dictionary with key : ([document, tf-idf]) because we already saved the idf in a dictioonary so the next code will be pretty fast in creating the dict. We save it a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionar3 = {}\n",
    "#length = 0\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    temp = []\n",
    "    for j in Docum_and_words[i].split():\n",
    "            if j not in temp:\n",
    "                #dicus = {}\n",
    "                code = diction[j]\n",
    "                value = tf(j, Docum_and_words[i].split())*idfdict[j]\n",
    "                li = [file, value]\n",
    "                if code not in dictionar3:\n",
    "                    #dicus[file]=value\n",
    "                    dictionar3[code] = [li]\n",
    "                else:   \n",
    "                    #dicus[file]=value\n",
    "                    dictionar3[code].append(li)\n",
    "                temp.append(j)\n",
    "import json\n",
    "\n",
    "with open('Dictionary2.json', 'w') as fp:\n",
    "    json.dump(dictionar3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114796"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionar3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I import the files I need and prepare the code for the search engine2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\dicturls.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "dicturls = json.loads(data) \n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction = json.loads(data) #first dict with every word and a unique value\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary1.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction2 = json.loads(data)#first inverted dict with number and list of document with a word that has that number in diction\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\ncontain.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "ncontain = json.loads(data) #dict with number of times a word appear in all the cod\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\idfdict.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "idfdict = json.loads(data) #dict with the idf for every word\n",
    "with open(r\"C:\\Users\\leona\\Desktop\\ADMHMK-3\\Dictionary2.json\", 'r') as file:\n",
    "    data = file.read()\n",
    "diction3 = json.loads(data) #second inverted dictionary wit doc and tfidf for eevry word\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASbout copsine similarity I have to take the tfidf of my k best document and multiply singularly it with the values of the quiery that are tf(relative of the quiery)*idf(relative to my documents for each component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The query entered by the user can also be represented as a vector. We will calculate the TF*IDF for the query\n",
    "\n",
    "TF\tIDF\tTF*IDF\n",
    "life\t0.5\t1.405507153\t0.702753576\n",
    "learning\t0.5\t1.405507153\t0.702753576\n",
    "Let us now calculate the cosine similarity of the query and Document1. You can do the calculation using this tool.\n",
    "\n",
    "Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "Dot product(Query, Document1) \n",
    "     = ((0.702753576) * (0.140550715) + (0.702753576)*(0.140550715))\n",
    "     = 0.197545035151\n",
    "\n",
    "||Query|| = sqrt((0.702753576)2 + (0.702753576)2) = 0.993843638185\n",
    "\n",
    "||Document1|| = sqrt((0.140550715)2 + (0.140550715)2) = 0.198768727354\n",
    "\n",
    "Cosine Similarity(Query, Document) = 0.197545035151 / (0.993843638185) * (0.198768727354)\n",
    "                                        = 0.197545035151 / 0.197545035151\n",
    "                                        = 1\n",
    "        \n",
    "https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prepare the code for the search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are the functions to easily compute the cosine similarity given two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pyplot\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def simple_dot(a, b):\n",
    "    dsum = 0.\n",
    "    for ((idx,), val) in np.ndenumerate(a):\n",
    "        dsum += float(val) * float(b[idx])\n",
    "    return dsum\n",
    "\n",
    "def l2_norm(a):\n",
    "    return math.sqrt(np.dot(a, a))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b) / (l2_norm(a)* l2_norm(b))\n",
    "np.dot([1,2],[3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remind that Docume_and_words is a list with inside every intro-plot(preprocessed) for every document, so I can just accees to it for my research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    " \n",
    "ps = PorterStemmer() \n",
    "#the fuction preprocess the string as asked in the hmk\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [ps.stem(w) for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENGINE2!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I build the search engine2. The heap sorting is in the 'cossim' list that will have all the possible values of cosine similarity we have found for the query. We will print the dataframe with the best 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubrick odyssey\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunshine (2007 film)</td>\n",
       "      <td>Sunshine               is a 2007     ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sunshine_(2007_f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Satellite in the Sky</td>\n",
       "      <td>Satellite in the Sky               is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Satellite_in_the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001: A Space Odyssey (film)</td>\n",
       "      <td>2001: A Space Odyssey               i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2001:_A_Space_Od...</td>\n",
       "      <td>0.9812423479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lolita (1962 film)</td>\n",
       "      <td>Lolita               is a 1962       ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lolita_(1962_film)</td>\n",
       "      <td>0.9512506956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010: The Year We Make Contact</td>\n",
       "      <td>2010: The Year We Make Contact       ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2010_(film)</td>\n",
       "      <td>0.9464950775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Title  \\\n",
       "0            Sunshine (2007 film)   \n",
       "1            Satellite in the Sky   \n",
       "2    2001: A Space Odyssey (film)   \n",
       "3              Lolita (1962 film)   \n",
       "4  2010: The Year We Make Contact   \n",
       "\n",
       "                                               Intro  \\\n",
       "0           Sunshine               is a 2007     ...   \n",
       "1           Satellite in the Sky               is...   \n",
       "2           2001: A Space Odyssey               i...   \n",
       "3           Lolita               is a 1962       ...   \n",
       "4           2010: The Year We Make Contact       ...   \n",
       "\n",
       "                                                 Url    Similarity  \n",
       "0  https://en.wikipedia.org/wiki/Sunshine_(2007_f...             1  \n",
       "1  https://en.wikipedia.org/wiki/Satellite_in_the...             1  \n",
       "2  https://en.wikipedia.org/wiki/2001:_A_Space_Od...  0.9812423479  \n",
       "3   https://en.wikipedia.org/wiki/Lolita_(1962_film)  0.9512506956  \n",
       "4          https://en.wikipedia.org/wiki/2010_(film)  0.9464950775  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HERE i DEFINE THE FUCTION TO RECEIVE THE DATASET, GIVEN AN INPUT\n",
    "import heapq as hq\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "y = list(input().split())\n",
    "def searchengine2(y):\n",
    "    if not y:\n",
    "        return print(\"You should must give me the string, errrrrrrror\")\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictiionary based on the input\n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "            new = []\n",
    "            for film in final_values:\n",
    "                if film in diction2[yfinal[codes]]:\n",
    "                    new.append(film)\n",
    "            final_values = new\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url', 'Similarity'])\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:  \n",
    "            lstofl = []\n",
    "            #here there is a lstofl that has vectors associated with every document, in order of final_values\n",
    "            for film in final_values:\n",
    "                item = []\n",
    "                for code in yfinal:\n",
    "                    for value in diction3[code]:\n",
    "                        if film in value:\n",
    "                            item.append(value[1])\n",
    "                            break\n",
    "                lstofl.append(item) \n",
    "            #print(lstofl)\n",
    "            #Now I have to create the inquiry vector and get the cosine similarity of beetween it and every component of lstofl \n",
    "            query = []\n",
    "            for i in y:\n",
    "                query.append(tf(i,y)*idfdict[i])\n",
    "            cossim = []\n",
    "            for vector in lstofl:\n",
    "                cossim.append(cosine_similarity(query, vector))\n",
    "            #print(cossim) #the cosine similariotyb in order of apparition of my document\n",
    "            dict_sim = {}\n",
    "            for indx in range(len(cossim)):\n",
    "                sim = cossim[indx]\n",
    "                if sim not in dict_sim:\n",
    "                    dict_sim[sim]=[final_values[indx]]\n",
    "                else:\n",
    "                    dict_sim[sim].append(final_values[indx])\n",
    "            #print(dict_sim)\n",
    "            Peak = 20\n",
    "            #HERE THE HEAP ALGORITHM\n",
    "            cossim = list(set(cossim))\n",
    "            to_select = hq.nlargest(Peak, cossim)   \n",
    "            k=0\n",
    "            #print(to_select)\n",
    "            #Now i have the name key(cossim) and values(docum) and I have to take the first 15 of them.\n",
    "            for i in to_select:\n",
    "                if(k<Peak):\n",
    "                    for document in dict_sim[i]:\n",
    "                            if(k>Peak):\n",
    "                                return megaDataframe\n",
    "                            totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                            totakeurl = str(int(totakeurl.replace('.tsv', '')))\n",
    "                            url = dicturls[totakeurl]\n",
    "                            Similarity = format(i, '.10g')\n",
    "                            temporary = pd.read_csv('Tsvfiles/'+'film'+(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                            title = temporary['title'][0]\n",
    "                            intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                            new_row = [title, intro, url, Similarity]\n",
    "                            megaDataframe.loc[k]=new_row\n",
    "                            k=k+1\n",
    "            return  megaDataframe\n",
    "A = searchengine2(y)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples! As we use simple strings and with not many repetitions it's possible more than one value will be one, as we will put a more difficult query we will have lower similarity probably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craig 007\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Casino Royale (2006 film)</td>\n",
       "      <td>Casino Royale               is a 2006...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Casino_Royale_(2...</td>\n",
       "      <td>0.8660544238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Title  \\\n",
       "0  Casino Royale (2006 film)   \n",
       "\n",
       "                                               Intro  \\\n",
       "0           Casino Royale               is a 2006...   \n",
       "\n",
       "                                                 Url    Similarity  \n",
       "0  https://en.wikipedia.org/wiki/Casino_Royale_(2...  0.8660544238  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daniel craig\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turner &amp; Hooch</td>\n",
       "      <td>Turner &amp; Hooch               is a 198...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Turner_%26_Hooch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Road to Perdition</td>\n",
       "      <td>Road to Perdition               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Road_to_Perdition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Defiance (2008 film)</td>\n",
       "      <td>Defiance               is a 2008 Amer...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Defiance_(2008_f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I Take This Oath</td>\n",
       "      <td>I Take This Oath               is a 1...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I_Take_This_Oath</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dangerous Passage</td>\n",
       "      <td>Dangerous Passage               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dangerous_Passage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sylvia (2003 film)</td>\n",
       "      <td>Sylvia               is a 2003 Britis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sylvia_(2003_film)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Layer Cake (film)</td>\n",
       "      <td>Layer Cake               (also occasi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Layer_Cake_(film)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Power of One (film)</td>\n",
       "      <td>The Power of One               is a 1...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Power_of_One...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>101 Dalmatians (1996 film)</td>\n",
       "      <td>101 Dalmatians               is a 199...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/101_Dalmatians_(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Thirteenth Floor</td>\n",
       "      <td>The Thirteenth Floor               is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Thirteenth_F...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lara Croft: Tomb Raider</td>\n",
       "      <td>Lara Croft: Tomb Raider              ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lara_Croft:_Tomb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Golden Compass (film)</td>\n",
       "      <td>The Golden Compass               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Golden_Compa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Invasion (film)</td>\n",
       "      <td>The Invasion               is a 2007 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Invasion_(film)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Adventures of Tintin (film)</td>\n",
       "      <td>The Adventures of Tintin             ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Adventures_o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dream House (2011 film)</td>\n",
       "      <td>Dream House               is a 2011 A...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dream_House_(201...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Finest Hours (2016 film)</td>\n",
       "      <td>The Finest Hours               is a 2...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Finest_Hours...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Logan Lucky</td>\n",
       "      <td>Logan Lucky               is a 2017 A...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Logan_Lucky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>No Time to Die</td>\n",
       "      <td>No Time to Die               is an up...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/No_Time_to_Die</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hit the Deck (1955 film)</td>\n",
       "      <td>Hit the Deck               is a 1955 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hit_the_Deck_(19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Dunwich Horror (film)</td>\n",
       "      <td>The Dunwich Horror               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Dunwich_Horr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Some Voices (film)</td>\n",
       "      <td>Some Voices               is a 2000 B...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Some_Voices_(film)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Title  \\\n",
       "0                    Turner & Hooch   \n",
       "1                 Road to Perdition   \n",
       "2              Defiance (2008 film)   \n",
       "3                  I Take This Oath   \n",
       "4                 Dangerous Passage   \n",
       "5                Sylvia (2003 film)   \n",
       "6                 Layer Cake (film)   \n",
       "7           The Power of One (film)   \n",
       "8        101 Dalmatians (1996 film)   \n",
       "9              The Thirteenth Floor   \n",
       "10          Lara Croft: Tomb Raider   \n",
       "11        The Golden Compass (film)   \n",
       "12              The Invasion (film)   \n",
       "13  The Adventures of Tintin (film)   \n",
       "14          Dream House (2011 film)   \n",
       "15     The Finest Hours (2016 film)   \n",
       "16                      Logan Lucky   \n",
       "17                   No Time to Die   \n",
       "18         Hit the Deck (1955 film)   \n",
       "19        The Dunwich Horror (film)   \n",
       "20               Some Voices (film)   \n",
       "\n",
       "                                                Intro  \\\n",
       "0            Turner & Hooch               is a 198...   \n",
       "1            Road to Perdition               is a ...   \n",
       "2            Defiance               is a 2008 Amer...   \n",
       "3            I Take This Oath               is a 1...   \n",
       "4            Dangerous Passage               is a ...   \n",
       "5            Sylvia               is a 2003 Britis...   \n",
       "6            Layer Cake               (also occasi...   \n",
       "7            The Power of One               is a 1...   \n",
       "8            101 Dalmatians               is a 199...   \n",
       "9            The Thirteenth Floor               is...   \n",
       "10           Lara Croft: Tomb Raider              ...   \n",
       "11           The Golden Compass               is a...   \n",
       "12           The Invasion               is a 2007 ...   \n",
       "13           The Adventures of Tintin             ...   \n",
       "14           Dream House               is a 2011 A...   \n",
       "15           The Finest Hours               is a 2...   \n",
       "16           Logan Lucky               is a 2017 A...   \n",
       "17           No Time to Die               is an up...   \n",
       "18           Hit the Deck               is a 1955 ...   \n",
       "19           The Dunwich Horror               is a...   \n",
       "20           Some Voices               is a 2000 B...   \n",
       "\n",
       "                                                  Url Similarity  \n",
       "0      https://en.wikipedia.org/wiki/Turner_%26_Hooch          1  \n",
       "1     https://en.wikipedia.org/wiki/Road_to_Perdition          1  \n",
       "2   https://en.wikipedia.org/wiki/Defiance_(2008_f...          1  \n",
       "3      https://en.wikipedia.org/wiki/I_Take_This_Oath          1  \n",
       "4     https://en.wikipedia.org/wiki/Dangerous_Passage          1  \n",
       "5    https://en.wikipedia.org/wiki/Sylvia_(2003_film)          1  \n",
       "6     https://en.wikipedia.org/wiki/Layer_Cake_(film)          1  \n",
       "7   https://en.wikipedia.org/wiki/The_Power_of_One...          1  \n",
       "8   https://en.wikipedia.org/wiki/101_Dalmatians_(...          1  \n",
       "9   https://en.wikipedia.org/wiki/The_Thirteenth_F...          1  \n",
       "10  https://en.wikipedia.org/wiki/Lara_Croft:_Tomb...          1  \n",
       "11  https://en.wikipedia.org/wiki/The_Golden_Compa...          1  \n",
       "12  https://en.wikipedia.org/wiki/The_Invasion_(film)          1  \n",
       "13  https://en.wikipedia.org/wiki/The_Adventures_o...          1  \n",
       "14  https://en.wikipedia.org/wiki/Dream_House_(201...          1  \n",
       "15  https://en.wikipedia.org/wiki/The_Finest_Hours...          1  \n",
       "16          https://en.wikipedia.org/wiki/Logan_Lucky          1  \n",
       "17       https://en.wikipedia.org/wiki/No_Time_to_Die          1  \n",
       "18  https://en.wikipedia.org/wiki/Hit_the_Deck_(19...          1  \n",
       "19  https://en.wikipedia.org/wiki/The_Dunwich_Horr...          1  \n",
       "20   https://en.wikipedia.org/wiki/Some_Voices_(film)          1  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know light film star\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adventures of Captain Marvel</td>\n",
       "      <td>Adventures of Captain Marvel         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adventures_of_Ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hour Before the Dawn</td>\n",
       "      <td>The Hour Before the Dawn             ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Hour_Before_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Journey into Light</td>\n",
       "      <td>Journey into Light               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Journey_into_Light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adventures of Captain Marvel</td>\n",
       "      <td>Adventures of Captain Marvel         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adventures_of_Ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hour Before the Dawn</td>\n",
       "      <td>The Hour Before the Dawn             ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Hour_Before_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Journey into Light</td>\n",
       "      <td>Journey into Light               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Journey_into_Light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adventures of Captain Marvel</td>\n",
       "      <td>Adventures of Captain Marvel         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adventures_of_Ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Hour Before the Dawn</td>\n",
       "      <td>The Hour Before the Dawn             ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Hour_Before_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Journey into Light</td>\n",
       "      <td>Journey into Light               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Journey_into_Light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Surviving the Game</td>\n",
       "      <td>Surviving the Game               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Surviving_the_Game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fools Rush In (1997 film)</td>\n",
       "      <td>Fools Rush In               is a 1997...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Fools_Rush_In_(1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Night of the Demons 3</td>\n",
       "      <td>Night of the Demons 3               (...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Night_of_the_Dem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stir of Echoes</td>\n",
       "      <td>Stir of Echoes               is a 199...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stir_of_Echoes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Corky Romano</td>\n",
       "      <td>Corky Romano               is a 2001 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Corky_Romano</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stolen (2012 film)</td>\n",
       "      <td>Stolen               , formerly known...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stolen_(2012_film)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>From Hell to Texas</td>\n",
       "      <td>From Hell to Texas               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/From_Hell_to_Texas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Silent Running</td>\n",
       "      <td>Silent Running               is a 197...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Silent_Running</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A Dark Song</td>\n",
       "      <td>A Dark Song               is a 2016 I...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A_Dark_Song</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Surviving the Game</td>\n",
       "      <td>Surviving the Game               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Surviving_the_Game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fools Rush In (1997 film)</td>\n",
       "      <td>Fools Rush In               is a 1997...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Fools_Rush_In_(1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Night of the Demons 3</td>\n",
       "      <td>Night of the Demons 3               (...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Night_of_the_Dem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title  \\\n",
       "0   Adventures of Captain Marvel   \n",
       "1       The Hour Before the Dawn   \n",
       "2             Journey into Light   \n",
       "3   Adventures of Captain Marvel   \n",
       "4       The Hour Before the Dawn   \n",
       "5             Journey into Light   \n",
       "6   Adventures of Captain Marvel   \n",
       "7       The Hour Before the Dawn   \n",
       "8             Journey into Light   \n",
       "9             Surviving the Game   \n",
       "10     Fools Rush In (1997 film)   \n",
       "11         Night of the Demons 3   \n",
       "12                Stir of Echoes   \n",
       "13                  Corky Romano   \n",
       "14            Stolen (2012 film)   \n",
       "15            From Hell to Texas   \n",
       "16                Silent Running   \n",
       "17                   A Dark Song   \n",
       "18            Surviving the Game   \n",
       "19     Fools Rush In (1997 film)   \n",
       "20         Night of the Demons 3   \n",
       "\n",
       "                                                Intro  \\\n",
       "0            Adventures of Captain Marvel         ...   \n",
       "1            The Hour Before the Dawn             ...   \n",
       "2            Journey into Light               is a...   \n",
       "3            Adventures of Captain Marvel         ...   \n",
       "4            The Hour Before the Dawn             ...   \n",
       "5            Journey into Light               is a...   \n",
       "6            Adventures of Captain Marvel         ...   \n",
       "7            The Hour Before the Dawn             ...   \n",
       "8            Journey into Light               is a...   \n",
       "9            Surviving the Game               is a...   \n",
       "10           Fools Rush In               is a 1997...   \n",
       "11           Night of the Demons 3               (...   \n",
       "12           Stir of Echoes               is a 199...   \n",
       "13           Corky Romano               is a 2001 ...   \n",
       "14           Stolen               , formerly known...   \n",
       "15           From Hell to Texas               is a...   \n",
       "16           Silent Running               is a 197...   \n",
       "17           A Dark Song               is a 2016 I...   \n",
       "18           Surviving the Game               is a...   \n",
       "19           Fools Rush In               is a 1997...   \n",
       "20           Night of the Demons 3               (...   \n",
       "\n",
       "                                                  Url Similarity  \n",
       "0   https://en.wikipedia.org/wiki/Adventures_of_Ca...          1  \n",
       "1   https://en.wikipedia.org/wiki/The_Hour_Before_...          1  \n",
       "2    https://en.wikipedia.org/wiki/Journey_into_Light          1  \n",
       "3   https://en.wikipedia.org/wiki/Adventures_of_Ca...          1  \n",
       "4   https://en.wikipedia.org/wiki/The_Hour_Before_...          1  \n",
       "5    https://en.wikipedia.org/wiki/Journey_into_Light          1  \n",
       "6   https://en.wikipedia.org/wiki/Adventures_of_Ca...          1  \n",
       "7   https://en.wikipedia.org/wiki/The_Hour_Before_...          1  \n",
       "8    https://en.wikipedia.org/wiki/Journey_into_Light          1  \n",
       "9    https://en.wikipedia.org/wiki/Surviving_the_Game          1  \n",
       "10  https://en.wikipedia.org/wiki/Fools_Rush_In_(1...          1  \n",
       "11  https://en.wikipedia.org/wiki/Night_of_the_Dem...          1  \n",
       "12       https://en.wikipedia.org/wiki/Stir_of_Echoes          1  \n",
       "13         https://en.wikipedia.org/wiki/Corky_Romano          1  \n",
       "14   https://en.wikipedia.org/wiki/Stolen_(2012_film)          1  \n",
       "15   https://en.wikipedia.org/wiki/From_Hell_to_Texas          1  \n",
       "16       https://en.wikipedia.org/wiki/Silent_Running          1  \n",
       "17          https://en.wikipedia.org/wiki/A_Dark_Song          1  \n",
       "18   https://en.wikipedia.org/wiki/Surviving_the_Game          1  \n",
       "19  https://en.wikipedia.org/wiki/Fools_Rush_In_(1...          1  \n",
       "20  https://en.wikipedia.org/wiki/Night_of_the_Dem...          1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disney movie 2019\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thumbelina (1994 film)</td>\n",
       "      <td>Thumbelina               (also known ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Avengers (2012 film)</td>\n",
       "      <td>Marvel's The Avengers                ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>Cars 3               is a 2017 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "      <td>0.94335701356643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frozen (2013 film)</td>\n",
       "      <td>Frozen               is a 2013 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "      <td>0.89654294215813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>The Chronicles of Narnia: Prince Casp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "      <td>0.82684193281763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0                    Thumbelina (1994 film)   \n",
       "1                  The Avengers (2012 film)   \n",
       "2                                    Cars 3   \n",
       "3                        Frozen (2013 film)   \n",
       "4  The Chronicles of Narnia: Prince Caspian   \n",
       "\n",
       "                                               Intro  \\\n",
       "0           Thumbelina               (also known ...   \n",
       "1           Marvel's The Avengers                ...   \n",
       "2           Cars 3               is a 2017 Americ...   \n",
       "3           Frozen               is a 2013 Americ...   \n",
       "4           The Chronicles of Narnia: Prince Casp...   \n",
       "\n",
       "                                                 Url        Similarity  \n",
       "0  https://en.wikipedia.org/wiki/Thumbelina_(1994...                 1  \n",
       "1  https://en.wikipedia.org/wiki/The_Avengers_(20...                 1  \n",
       "2               https://en.wikipedia.org/wiki/Cars_3  0.94335701356643  \n",
       "3   https://en.wikipedia.org/wiki/Frozen_(2013_film)  0.89654294215813  \n",
       "4  https://en.wikipedia.org/wiki/The_Chronicles_o...  0.82684193281763  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light light light film sun\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-Facial</td>\n",
       "      <td>Multi-Facial               is a 1995 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Multi-Facial</td>\n",
       "      <td>0.9999935714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Legend (1985 film)</td>\n",
       "      <td>Legend               is a 1985 Americ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Legend_(1985_film)</td>\n",
       "      <td>0.999985536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blade II</td>\n",
       "      <td>Blade II               is a 2002 Amer...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Blade_II</td>\n",
       "      <td>0.9999598243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Close Encounters of the Third Kind</td>\n",
       "      <td>Close Encounters of the Third Kind   ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Close_Encounters...</td>\n",
       "      <td>0.9998698517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Little Monsters</td>\n",
       "      <td>Little Monsters               is a 19...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Little_Monsters</td>\n",
       "      <td>0.9980234564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Midnight Special (film)</td>\n",
       "      <td>Midnight Special               is a 2...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Midnight_Special...</td>\n",
       "      <td>0.9980109468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pitch Black (film)</td>\n",
       "      <td>Pitch Black               (titled    ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Pitch_Black_(film)</td>\n",
       "      <td>0.9980109468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Hallow</td>\n",
       "      <td>The Hallow               (originally ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Hallow</td>\n",
       "      <td>0.9948607933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>League of Gods</td>\n",
       "      <td>League of Gods               (       ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/League_of_Gods</td>\n",
       "      <td>0.9928853908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Wraith</td>\n",
       "      <td>The Wraith               is a 1986   ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Wraith</td>\n",
       "      <td>0.9928681177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Home (2015 film)</td>\n",
       "      <td>Home               is a 2015 American...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Home_(2015_anima...</td>\n",
       "      <td>0.9927539708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Curious George (film)</td>\n",
       "      <td>Curious George               is a 200...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Curious_George_(...</td>\n",
       "      <td>0.9927539708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pretty Woman</td>\n",
       "      <td>Pretty Woman               is a 1990 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Pretty_Woman</td>\n",
       "      <td>0.9927088611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hocus Pocus (1993 film)</td>\n",
       "      <td>Hocus Pocus               is a 1993 A...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hocus_Pocus_(199...</td>\n",
       "      <td>0.9925987732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Megiddo: The Omega Code 2</td>\n",
       "      <td>Megiddo: The Omega Code 2            ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Megiddo:_The_Ome...</td>\n",
       "      <td>0.9918982343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Tree of Life (film)</td>\n",
       "      <td>The Tree of Life               is a 2...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Tree_of_Life...</td>\n",
       "      <td>0.9739265829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A Far Off Place</td>\n",
       "      <td>A Far Off Place               (aka   ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A_Far_Off_Place</td>\n",
       "      <td>0.9159937312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I Love You, Beth Cooper (film)</td>\n",
       "      <td>I Love You, Beth Cooper              ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I_Love_You,_Beth...</td>\n",
       "      <td>0.9159937312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Atlantis, the Lost Continent</td>\n",
       "      <td>Atlantis, the Lost Continent         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Atlantis,_the_Lo...</td>\n",
       "      <td>0.9159937312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I Love You, Beth Cooper (film)</td>\n",
       "      <td>I Love You, Beth Cooper              ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I_Love_You,_Beth...</td>\n",
       "      <td>0.9159937312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Atlantis, the Lost Continent</td>\n",
       "      <td>Atlantis, the Lost Continent         ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Atlantis,_the_Lo...</td>\n",
       "      <td>0.9159937312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title  \\\n",
       "0                         Multi-Facial   \n",
       "1                   Legend (1985 film)   \n",
       "2                             Blade II   \n",
       "3   Close Encounters of the Third Kind   \n",
       "4                      Little Monsters   \n",
       "5              Midnight Special (film)   \n",
       "6                   Pitch Black (film)   \n",
       "7                           The Hallow   \n",
       "8                       League of Gods   \n",
       "9                           The Wraith   \n",
       "10                    Home (2015 film)   \n",
       "11               Curious George (film)   \n",
       "12                        Pretty Woman   \n",
       "13             Hocus Pocus (1993 film)   \n",
       "14           Megiddo: The Omega Code 2   \n",
       "15             The Tree of Life (film)   \n",
       "16                     A Far Off Place   \n",
       "17      I Love You, Beth Cooper (film)   \n",
       "18        Atlantis, the Lost Continent   \n",
       "19      I Love You, Beth Cooper (film)   \n",
       "20        Atlantis, the Lost Continent   \n",
       "\n",
       "                                                Intro  \\\n",
       "0            Multi-Facial               is a 1995 ...   \n",
       "1            Legend               is a 1985 Americ...   \n",
       "2            Blade II               is a 2002 Amer...   \n",
       "3            Close Encounters of the Third Kind   ...   \n",
       "4            Little Monsters               is a 19...   \n",
       "5            Midnight Special               is a 2...   \n",
       "6            Pitch Black               (titled    ...   \n",
       "7            The Hallow               (originally ...   \n",
       "8            League of Gods               (       ...   \n",
       "9            The Wraith               is a 1986   ...   \n",
       "10           Home               is a 2015 American...   \n",
       "11           Curious George               is a 200...   \n",
       "12           Pretty Woman               is a 1990 ...   \n",
       "13           Hocus Pocus               is a 1993 A...   \n",
       "14           Megiddo: The Omega Code 2            ...   \n",
       "15           The Tree of Life               is a 2...   \n",
       "16           A Far Off Place               (aka   ...   \n",
       "17           I Love You, Beth Cooper              ...   \n",
       "18           Atlantis, the Lost Continent         ...   \n",
       "19           I Love You, Beth Cooper              ...   \n",
       "20           Atlantis, the Lost Continent         ...   \n",
       "\n",
       "                                                  Url    Similarity  \n",
       "0          https://en.wikipedia.org/wiki/Multi-Facial  0.9999935714  \n",
       "1    https://en.wikipedia.org/wiki/Legend_(1985_film)   0.999985536  \n",
       "2              https://en.wikipedia.org/wiki/Blade_II  0.9999598243  \n",
       "3   https://en.wikipedia.org/wiki/Close_Encounters...  0.9998698517  \n",
       "4       https://en.wikipedia.org/wiki/Little_Monsters  0.9980234564  \n",
       "5   https://en.wikipedia.org/wiki/Midnight_Special...  0.9980109468  \n",
       "6    https://en.wikipedia.org/wiki/Pitch_Black_(film)  0.9980109468  \n",
       "7            https://en.wikipedia.org/wiki/The_Hallow  0.9948607933  \n",
       "8        https://en.wikipedia.org/wiki/League_of_Gods  0.9928853908  \n",
       "9            https://en.wikipedia.org/wiki/The_Wraith  0.9928681177  \n",
       "10  https://en.wikipedia.org/wiki/Home_(2015_anima...  0.9927539708  \n",
       "11  https://en.wikipedia.org/wiki/Curious_George_(...  0.9927539708  \n",
       "12         https://en.wikipedia.org/wiki/Pretty_Woman  0.9927088611  \n",
       "13  https://en.wikipedia.org/wiki/Hocus_Pocus_(199...  0.9925987732  \n",
       "14  https://en.wikipedia.org/wiki/Megiddo:_The_Ome...  0.9918982343  \n",
       "15  https://en.wikipedia.org/wiki/The_Tree_of_Life...  0.9739265829  \n",
       "16      https://en.wikipedia.org/wiki/A_Far_Off_Place  0.9159937312  \n",
       "17  https://en.wikipedia.org/wiki/I_Love_You,_Beth...  0.9159937312  \n",
       "18  https://en.wikipedia.org/wiki/Atlantis,_the_Lo...  0.9159937312  \n",
       "19  https://en.wikipedia.org/wiki/I_Love_You,_Beth...  0.9159937312  \n",
       "20  https://en.wikipedia.org/wiki/Atlantis,_the_Lo...  0.9159937312  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input().split())\n",
    "searchengine2(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex[3] build a new engine with a personal Score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lone ladi'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "titles = {}\n",
    "for i in range(50):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    datatemp = pd.read_csv(file, delimiter = '\\t')\n",
    "    title =  datatemp['title'][0]\n",
    "    titles[file]=title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "Docum_and_words = []\n",
    "for i in range(30000):\n",
    "    file = \"Cleantsv/filmclean-\"+str(i)+'.tsv'\n",
    "    file1 = open(file, encoding=\"utf8\") \n",
    "    line = file1.read()\n",
    "    words = line.split('\\t') \n",
    "    wordssplitted1 = words[14].split()\n",
    "    wordssplitted2 = words[15].split()\n",
    "    words = wordssplitted1+wordssplitted2\n",
    "    A = \" \".join(words)\n",
    "    Docum_and_words.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(Docum_and_words[0].split())\n",
    "total = 0\n",
    "for i in Docum_and_words:\n",
    "    total = total + len(i.split())\n",
    "average = float(total)/30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266.1692"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def splitTextToDouble(string):\n",
    "    words = string.split()\n",
    "    grouped_words = [' '.join(words[i: i + 2]) for i in range(0, len(words), 2)]\n",
    "    return grouped_words\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc.split())\n",
    "\n",
    "def score_function(doc, lizt, z, x, d):\n",
    "    average = 266\n",
    "    score = 0\n",
    "    half = 0\n",
    "    diri = 0\n",
    "    da = pd.read_csv(doc, delimiter = '\\t')\n",
    "    intro_plot = da['plot'][0]+da['intro'][0]\n",
    "    for i in list(set(lizt)):\n",
    "        score = score + tf(i, intro_plot)\n",
    "    ln = len(lizt)\n",
    "    for i in range(ln):\n",
    "        #print(da)\n",
    "        if lizt[i] in da['title'][0].split() and x == 'n':\n",
    "            score = score + 1\n",
    "        if ln>1 and i < ln-1:\n",
    "            if x =='ye' and lizt[i]+' '+lizt[i+1] in splitTextToDouble(da['starring'][0]):\n",
    "                half = half + 25\n",
    "            if d == 'ye' and lizt[i]+' '+lizt[i+1] in splitTextToDouble(da['director'][0]):\n",
    "                diri = diri + 20\n",
    "        elif ln<=1:\n",
    "            if lizt[i] in da['starring'][0].split():\n",
    "                half = half + 5\n",
    "            if lizt[i] in da['director'][0].split():\n",
    "                diri = diri + 5\n",
    "        lizt = list(set(lizt))\n",
    "        for i in lizt:\n",
    "            if i in da['starring'][0]:\n",
    "                score = score + 3\n",
    "            if i in da['director'][0]:\n",
    "                score = score + 6\n",
    "    #print(half)\n",
    "    if z == da['language'][0]:\n",
    "        score = score + 4\n",
    "    #print(x)\n",
    "    if x == 'ye':\n",
    "        score = score + half\n",
    "    value = len(intro_plot.split())\n",
    "    if value >= average+90:\n",
    "        score = score + 4\n",
    "    if value < (average):\n",
    "        score = score -4\n",
    "    na = 0\n",
    "    for i in da.values[0]:\n",
    "        if i == 'na':\n",
    "            na = na +1\n",
    "    if na > 3:\n",
    "        score = score - 4\n",
    "    #print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in search engine3, write a string and, after some more questions I will give you a very likely film you can be looking for!\n",
      "john wayne western\n",
      "Write your preferred language: \n",
      "english\n",
      "You gave an actor/actress name in the string? type ye or n \n",
      "ye\n",
      "You gave also a director name in the string?type ye or n \n",
      "n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Undefeated (1969 film)</td>\n",
       "      <td>The Undefeated               is a 196...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Undefeated_(...</td>\n",
       "      <td>112.003157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Comancheros (film)</td>\n",
       "      <td>The Comancheros               is a 19...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Comancheros_...</td>\n",
       "      <td>104.004345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hondo (film)</td>\n",
       "      <td>Hondo               is a 1953        ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hondo_(film)</td>\n",
       "      <td>94.004319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stagecoach (1939 film)</td>\n",
       "      <td>Stagecoach               is a 1939 Am...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stagecoach_(1939...</td>\n",
       "      <td>94.003769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fort Apache (film)</td>\n",
       "      <td>Fort Apache               is a 1948 A...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Fort_Apache_(film)</td>\n",
       "      <td>94.003196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rio Grande (film)</td>\n",
       "      <td>Rio Grande               is a 1950   ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rio_Grande_(film)</td>\n",
       "      <td>94.002823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Searchers</td>\n",
       "      <td>The Searchers               is a 1956...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Searchers_(f...</td>\n",
       "      <td>94.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Man Who Shot Liberty Valance</td>\n",
       "      <td>The Man Who Shot Liberty Valance     ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Man_Who_Shot...</td>\n",
       "      <td>94.001994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Title  \\\n",
       "0        The Undefeated (1969 film)   \n",
       "1            The Comancheros (film)   \n",
       "2                      Hondo (film)   \n",
       "3            Stagecoach (1939 film)   \n",
       "4                Fort Apache (film)   \n",
       "5                 Rio Grande (film)   \n",
       "6                     The Searchers   \n",
       "7  The Man Who Shot Liberty Valance   \n",
       "\n",
       "                                               Intro  \\\n",
       "0           The Undefeated               is a 196...   \n",
       "1           The Comancheros               is a 19...   \n",
       "2           Hondo               is a 1953        ...   \n",
       "3           Stagecoach               is a 1939 Am...   \n",
       "4           Fort Apache               is a 1948 A...   \n",
       "5           Rio Grande               is a 1950   ...   \n",
       "6           The Searchers               is a 1956...   \n",
       "7           The Man Who Shot Liberty Valance     ...   \n",
       "\n",
       "                                                 Url       Score  \n",
       "0  https://en.wikipedia.org/wiki/The_Undefeated_(...  112.003157  \n",
       "1  https://en.wikipedia.org/wiki/The_Comancheros_...  104.004345  \n",
       "2         https://en.wikipedia.org/wiki/Hondo_(film)   94.004319  \n",
       "3  https://en.wikipedia.org/wiki/Stagecoach_(1939...   94.003769  \n",
       "4   https://en.wikipedia.org/wiki/Fort_Apache_(film)   94.003196  \n",
       "5    https://en.wikipedia.org/wiki/Rio_Grande_(film)   94.002823  \n",
       "6  https://en.wikipedia.org/wiki/The_Searchers_(f...   94.002398  \n",
       "7  https://en.wikipedia.org/wiki/The_Man_Who_Shot...   94.001994  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HERE i DEFINE THE FUNCTION TO RECEIVE THE DATASET, GIVEN AN INPUT\n",
    "import heapq as hq\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "y = list(input('Welcome in search engine3, write a string and, after some more questions I will give you a very likely film you can be looking for!\\n').split())\n",
    "z = input('Write your preferred language: \\n')\n",
    "x = input('You gave an actor/actress name in the string? type ye or n \\n')\n",
    "d = input('You gave also a director name in the string?type ye or n \\n')\n",
    "def searchengine3(y, z, x, d):\n",
    "    if not y:\n",
    "        return print(\"You should must give me the string, errrrrrrror\")\n",
    "    for i in range(len(y)):\n",
    "        y[i]= preprocess(str(y[i]))\n",
    "    #Now I tranform the list of input in a list of the codes in the dictiionary based on the input\n",
    "    yfinal=[] #use this because some words have no match in the vocabulary\n",
    "    for i in range(len(y)):\n",
    "        #print(y[i])\n",
    "        if y[i] in diction:\n",
    "            yfinal.append(diction[y[i]])\n",
    "    #Now I have to search inside the lists of values from the keys i foundb and see if some films match in the various keys.\n",
    "    if  len(yfinal)<len(y):\n",
    "        return print('We are sorry there are no films, in my database, that match ALL the words you gave me !(')\n",
    "    else:\n",
    "        starting_values = diction2[yfinal[0]]\n",
    "        final_values = starting_values.copy()\n",
    "        for codes in range(1,len(yfinal)):\n",
    "            new = []\n",
    "            for film in final_values:\n",
    "                if film in diction2[yfinal[codes]]:\n",
    "                    new.append(film)\n",
    "            final_values = new\n",
    "        megaDataframe = pd.DataFrame(columns = ['Title', 'Intro', 'Url', 'Score'])\n",
    "        #print(final_values)\n",
    "        if not final_values:\n",
    "            return print(\"Wow no film matched my quiery, I need more films to compare!\")\n",
    "        else:  \n",
    "            dict_score = {}\n",
    "            score_list = []\n",
    "            Threshold  = 8\n",
    "            k_limit = 12\n",
    "            #print(final_values)\n",
    "            for doc in final_values:\n",
    "                #score_function is the function I create and used to give points to every film\n",
    "                x = preprocess(x)\n",
    "                z = preprocess(z)\n",
    "                d = preprocess(d)\n",
    "                score = score_function(doc, y, z, x, d)\n",
    "               # print(score)\n",
    "                if score not in dict_score:\n",
    "                    dict_score[score]=[doc]\n",
    "                    score_list.append(score)\n",
    "                else:\n",
    "                    dict_score[score].append(doc)\n",
    "            score_list = list(set(score_list))\n",
    "            #print(dict_score)\n",
    "            best = hq.nlargest(Threshold, score_list)\n",
    "            k = 0\n",
    "            #print(best)\n",
    "            for score in best:\n",
    "                #print(score)\n",
    "                if k < k_limit :\n",
    "                    for document in dict_score[score]:\n",
    "                        #print(document)\n",
    "                        if k < k_limit:\n",
    "                            totakeurl = document.replace('Cleantsv/filmclean-','')\n",
    "                            totakeurl = str(int(totakeurl.replace('.tsv', '')))\n",
    "                            url = dicturls[totakeurl]\n",
    "                            Score = score\n",
    "                            temporary = pd.read_csv('Tsvfiles/'+'film'+(totakeurl)+'.tsv',delimiter='\\t' )\n",
    "                            title = temporary['title'][0]\n",
    "                            intro =  temporary['intro'][0].replace('\\r\\n','')\n",
    "                            new_row = [title, intro, url, Score]\n",
    "                            megaDataframe.loc[k]=new_row \n",
    "                            #print(megaDataframe)\n",
    "                            k = k+1\n",
    "                        else:\n",
    "                            return megaDataframe\n",
    "                else:\n",
    "                    return megaDataframe\n",
    "            return megaDataframe\n",
    "                \n",
    "            \n",
    "A = searchengine3(y, z, x, d)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in search engine3, write a string and, after some more questions I will give you a very likely film you can be looking for!\n",
      "anne gothic\n",
      "Write your preferred language: \n",
      "\n",
      "You gave an actor/actress name in the string? type ye or n \n",
      "\n",
      "You gave also a director name in the string?type ye or n \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Black Torment</td>\n",
       "      <td>The Black Torment               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Black_Torment</td>\n",
       "      <td>10.004171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Others (2001 film)</td>\n",
       "      <td>The Others               (           ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Others_(2001...</td>\n",
       "      <td>10.002191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So Evil My Love</td>\n",
       "      <td>So Evil My Love               is a 19...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/So_Evil_My_Love</td>\n",
       "      <td>10.001192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Interview with the Vampire (film)</td>\n",
       "      <td>Interview with the Vampire           ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Interview_with_t...</td>\n",
       "      <td>4.000661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Red Kingdom Rising</td>\n",
       "      <td>Red Kingdom Rising               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Red_Kingdom_Rising</td>\n",
       "      <td>-3.990826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title  \\\n",
       "0                  The Black Torment   \n",
       "1             The Others (2001 film)   \n",
       "2                    So Evil My Love   \n",
       "3  Interview with the Vampire (film)   \n",
       "4                 Red Kingdom Rising   \n",
       "\n",
       "                                               Intro  \\\n",
       "0           The Black Torment               is a ...   \n",
       "1           The Others               (           ...   \n",
       "2           So Evil My Love               is a 19...   \n",
       "3           Interview with the Vampire           ...   \n",
       "4           Red Kingdom Rising               is a...   \n",
       "\n",
       "                                                 Url      Score  \n",
       "0    https://en.wikipedia.org/wiki/The_Black_Torment  10.004171  \n",
       "1  https://en.wikipedia.org/wiki/The_Others_(2001...  10.002191  \n",
       "2      https://en.wikipedia.org/wiki/So_Evil_My_Love  10.001192  \n",
       "3  https://en.wikipedia.org/wiki/Interview_with_t...   4.000661  \n",
       "4   https://en.wikipedia.org/wiki/Red_Kingdom_Rising  -3.990826  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input('Welcome in search engine3, write a string and, after some more questions I will give you a very likely film you can be looking for!\\n').split())\n",
    "z = input('Write your preferred language: \\n')\n",
    "x = input('You gave an actor/actress name in the string? type ye or n \\n')\n",
    "d = input('You gave also a director name in the string?type ye or n \\n')\n",
    "searchengine3(y, z, x, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in search engine3, write a string and, after some more questions I will give you a very likely film you can be looking for!\n",
      "kubrick film star\n",
      "Write your preferred language: \n",
      "english\n",
      "You gave an actor/actress name in the string? type ye or n \n",
      "n\n",
      "You gave also a director name in the string?type ye or n \n",
      "ye\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001: A Space Odyssey (film)</td>\n",
       "      <td>2001: A Space Odyssey               i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2001:_A_Space_Od...</td>\n",
       "      <td>26.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Metal Jacket</td>\n",
       "      <td>Full Metal Jacket               is a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Full_Metal_Jacket</td>\n",
       "      <td>26.003464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Shining (film)</td>\n",
       "      <td>The Shining               is a 1980  ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Shining_(film)</td>\n",
       "      <td>26.003232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lolita (1962 film)</td>\n",
       "      <td>Lolita               is a 1962       ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lolita_(1962_film)</td>\n",
       "      <td>26.003016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barry Lyndon</td>\n",
       "      <td>Barry Lyndon               is a 1975 ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Barry_Lyndon</td>\n",
       "      <td>26.002542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Spartacus (film)</td>\n",
       "      <td>Spartacus               is a 1960 Ame...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Spartacus_(film)</td>\n",
       "      <td>26.002525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paths of Glory</td>\n",
       "      <td>Paths of Glory               is a 195...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Paths_of_Glory</td>\n",
       "      <td>26.002489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dr. Strangelove</td>\n",
       "      <td>Dr. Strangelove or: How I Learned to ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dr._Strangelove</td>\n",
       "      <td>26.002049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dr. Strangelove</td>\n",
       "      <td>Dr. Strangelove or: How I Learned to ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dr._Strangelove_...</td>\n",
       "      <td>26.002049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Title  \\\n",
       "0  2001: A Space Odyssey (film)   \n",
       "1             Full Metal Jacket   \n",
       "2            The Shining (film)   \n",
       "3            Lolita (1962 film)   \n",
       "4                  Barry Lyndon   \n",
       "5              Spartacus (film)   \n",
       "6                Paths of Glory   \n",
       "7               Dr. Strangelove   \n",
       "8               Dr. Strangelove   \n",
       "\n",
       "                                               Intro  \\\n",
       "0           2001: A Space Odyssey               i...   \n",
       "1           Full Metal Jacket               is a ...   \n",
       "2           The Shining               is a 1980  ...   \n",
       "3           Lolita               is a 1962       ...   \n",
       "4           Barry Lyndon               is a 1975 ...   \n",
       "5           Spartacus               is a 1960 Ame...   \n",
       "6           Paths of Glory               is a 195...   \n",
       "7           Dr. Strangelove or: How I Learned to ...   \n",
       "8           Dr. Strangelove or: How I Learned to ...   \n",
       "\n",
       "                                                 Url      Score  \n",
       "0  https://en.wikipedia.org/wiki/2001:_A_Space_Od...  26.004360  \n",
       "1    https://en.wikipedia.org/wiki/Full_Metal_Jacket  26.003464  \n",
       "2   https://en.wikipedia.org/wiki/The_Shining_(film)  26.003232  \n",
       "3   https://en.wikipedia.org/wiki/Lolita_(1962_film)  26.003016  \n",
       "4         https://en.wikipedia.org/wiki/Barry_Lyndon  26.002542  \n",
       "5     https://en.wikipedia.org/wiki/Spartacus_(film)  26.002525  \n",
       "6       https://en.wikipedia.org/wiki/Paths_of_Glory  26.002489  \n",
       "7      https://en.wikipedia.org/wiki/Dr._Strangelove  26.002049  \n",
       "8  https://en.wikipedia.org/wiki/Dr._Strangelove_...  26.002049  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input('Welcome in search engine3, write a string and, after some more questions I will give you a very likely film you can be looking for!\\n').split())\n",
    "z = input('Write your preferred language: \\n')\n",
    "x = input('You gave an actor/actress name in the string? type ye or n \\n')\n",
    "d = input('You gave also a director name in the string?type ye or n \\n')\n",
    "searchengine3(y, z, x, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in search engine3, write a string and, after some more questions I will give you a very likely film you can be looking for!\n",
      "sioux costner\n",
      "Write your preferred language: \n",
      "english\n",
      "You gave an actor/actress name in the string? type ye or n \n",
      "ye\n",
      "You gave also a director name in the string?type ye or n \n",
      "ye\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dances with Wolves</td>\n",
       "      <td>Dances with Wolves               is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dances_with_Wolves</td>\n",
       "      <td>22.00356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Title                                              Intro  \\\n",
       "0  Dances with Wolves           Dances with Wolves               is a...   \n",
       "\n",
       "                                                Url     Score  \n",
       "0  https://en.wikipedia.org/wiki/Dances_with_Wolves  22.00356  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(input('Welcome in search engine3, write a string and, after some more questions I will give you a very likely film you can be looking for!\\n').split())\n",
    "z = input('Write your preferred language: \\n')\n",
    "x = input('You gave an actor/actress name in the string? type ye or n \\n')\n",
    "d = input('You gave also a director name in the string?type ye or n \\n')\n",
    "searchengine3(y, z, x, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex[4] Algorithmic question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the LPS is 7\n"
     ]
    }
   ],
   "source": [
    "# Returns the length of the longest  \n",
    "# palindromic subsequence in seq  \n",
    "def lps(seq, i, j): \n",
    "      \n",
    "    # Base Case 1: If there is only 1 character  \n",
    "    if (i == j): \n",
    "        return 1\n",
    "  \n",
    "    # Base Case 2: If there are only 2 character and both are same\n",
    "    if (seq[i] == seq[j] and i + 1 == j):\n",
    "        return 2\n",
    "      \n",
    "    # If the first and last characters match  \n",
    "    if (seq[i] == seq[j]): \n",
    "        return lps(seq, i + 1, j - 1) + 2\n",
    "  \n",
    "    # If the first and last characters do not match \n",
    "    return max(lps(seq, i, j - 1),  \n",
    "               lps(seq, i + 1, j)) \n",
    "  \n",
    "# Code to run the program\n",
    "if __name__ == '__main__': \n",
    "    seq = \"DATAMININGSAPIENZA\"\n",
    "    seq = seq.lower()\n",
    "    n = len(seq) \n",
    "    print(\"The length of the LPS is\",lps(seq, 0, n - 1)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
